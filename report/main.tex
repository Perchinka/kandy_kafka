\documentclass[10pt , a4paper]{report}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \usepackage[T2A,T1]{fontenc}
\usepackage{silence}

\usepackage{pmboxdraw}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{sectsty}
\usepackage{xcolor}
\usepackage{appendix}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[utf8x]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{cmap}

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

%paragraphs params
\setlength{\oddsidemargin}{0cm}
\setlength{\textheight}{24cm}
\setlength{\textwidth}{16cm}
\setlength{\topmargin}{-2cm}
\renewcommand{\baselinestretch}{1.1}

% fonts
\usepackage{times}
\usepackage{helvet} 
\usepackage{sectsty}
\allsectionsfont{\sffamily}
\renewcommand{\familydefault}{\sfdefault}


%listings
\lstset{basicstyle=\ttfamily\footnotesize}
\lstset{frame=tlrb}
\usepackage{listings}

\usepackage{minted}
\usepackage{forest}

\usepackage{adjustbox}
\usepackage{subcaption}
\captionsetup{compatibility=false}

\newenvironment{code}{\captionsetup{type=listing}}{}

\setminted{
    breaklines=true,
    frame=single,
    framesep=5pt
}

\begin{document}

\title{Computer Science Project\\
        \textit{Kandy: CLI tool for managing Kafka}}

\author{\textbf{Alex Lukin}\\
Center: 22109\\
Student: 009961\\
Chesterton Community College\\
\textit{Cambridge, United Kingdom}\\}


\maketitle

\newpage
\tableofcontents
\newpage

\chapter{Analysis}
\section{Problem Definition}

Apache Kafka\footnote{\textbf{Kafka} is a distributed streaming platform designed for processing, storing, and transmitting data in real-time. It provides a reliable mechanism for data streaming, enabling applications and services to exchange information and respond to changes instantly. Kafka is built to handle enormous volumes of data, offering high performance and fault tolerance through its distributed architecture.} has become a crucial system for handling real-time data streams in today’s data-driven landscape. Despite Kafka’s widespread adoption, there remains a lack of streamlined tools for efficient management of Kafka instances directly from the command line. This gap poses a challenge for developers who need quick, straightforward ways to interact with Kafka without getting slowed down by complex setup processes.

While web applications can bridge this gap by providing intuitive interfaces, they are typically hosted in Docker containers. Docker’s benefits of portability and consistency across different environments are valuable, yet setting up these applications within large systems can become cumbersome and time-consuming, especially for quick local tests.

In summary, Docker facilitates deploying web applications with user-friendly interfaces for Kafka management, but its setup can be daunting within larger infrastructures. To address this, the proposed solution is a dedicated Text-Based User Interface (TUI) utility aimed at simplifying Kafka management, ultimately boosting developer productivity.

The project is named \textbf{Kandy}(Stands for Handy Kafka) and is envisioned as a TUI tool specifically designed for Kafka management

\section{Stakeholders} 
The Kandy project targets three primary stakeholder groups, each with distinct needs that Kandy will address:

\begin{itemize} 
    \item \textbf{Administrators} who oversee Kafka performance, data integrity, and cluster health. Kandy’s real-time monitoring features provide quick insights into cluster status, helping administrators maintain stability and react swiftly to any issues.
    \item \textbf{Developers} who integrate Kafka into their applications and require efficient access to Kafka for testing, debugging, and troubleshooting. Kandy streamlines access to critical metrics, enhancing their productivity and reducing setup time for Kafka-related tasks.
    \item \textbf{DevOps teams} responsible for Kafka cluster infrastructure, uptime, and stability. Kandy’s intuitive interface and essential monitoring tools enable DevOps teams to oversee cluster health without the need for complex GUI-based solutions, making management tasks more efficient.
\end{itemize}

To represent the target audience, I have identified a key user persona: \textbf{Sarah}, the Head of Engineering at Valerann. Sarah oversees multiple data-driven projects that rely on real-time Kafka streams, frequently monitoring consumer lags, topic performance, and system stability to ensure smooth operation for her teams. With extensive experience in both development and Kafka management, Sarah’s insights are invaluable for evaluating Kandy’s effectiveness. Sarah’s feedback will help me to be sure if Kandy meets the practical needs of it's users.

\newpage

\section{Kafka Architecture}\label{Kafka_architecture}

Throughout whole project, I will use terminology specific to Kafka’s architecture to describe its components and functionalities. So
this subsection dives into the foundational concepts of Kafka. For better understanding the key ideas behind Kafka and its functionality, let’s break it down into its core components and how they work together.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/KafkaStructureModel.png}
    \caption{Key Components of Kafka Architecture}
    \label{fig:kafka_arch}
\end{figure}

\subsubsection*{Core Components of Kafka}
\begin{itemize}
    \item \textbf{Message:} The fundamental unit of data in Kafka. It consists of a key, value, and optional metadata (headers)
    
    \textit{Example:} A message in the \textit{Weather\_Updates} topic might include a key representing a city ID, a value containing temperature data, and headers indicating the time of measurement.
    
    \item \textbf{Producers:}  
    These are the data generators. Producers are applications or systems responsible for sending messages to Kafka topics. Think of them as the "writers" in Kafka's ecosystem. 
    
    \textit{Example:} A weather monitoring application sending real-time temperature updates acts as a producer.

    \item \textbf{Consumers:}  
    Consumers retrieve and process data stored in Kafka topics. They are the "readers" that transform raw data into actionable insights or visualizations.  
    
    \textit{Example:} A live dashboard showing weather updates would be a consumer.

    \item \textbf{Topics and Partitions:}  
    \begin{itemize}
        \item \textbf{Topics:} Logical containers that organize messages by category. For instance, a topic named \textit{Weather\_Updates} might store all weather-related data.
        \item \textbf{Partitions:} Subsections of a topic that allow for parallel processing. Each partition stores messages in an immutable, sequential order. This setup enables Kafka to scale efficiently by distributing messages across multiple servers (brokers).
    \end{itemize}

    \item \textbf{Brokers:}  
    Brokers are Kafka's backbone. They store data and manage partitions. When combined, multiple brokers form a Kafka cluster, which offers high availability and fault tolerance by replicating and balancing data across servers.

    \item \textbf{Zookeeper:}  
    Zookeeper is a distributed coordination service crucial to Kafka's operations. It tracks metadata, manages partition leadership, and monitors broker health to ensure cluster stability. Although Zookeeper is essential in Kafka’s traditional setup, newer Kafka versions are moving towards its replacement with Kafka's internal \textit{KRaft} system.  
    \textit{(Note: I'm not planning on working with Zookeeper directly in this project, but understanding its role might be helpful)}
\end{itemize}

\subsubsection*{How Kafka Works}
\begin{enumerate}
    \item \textbf{Producers Generate Data:}  
    Applications acting as producers send data to Kafka, specifying the relevant topic. For instance, a weather app might publish updates to a \textit{Weather\_Updates} topic.

    \item \textbf{Kafka Distributes Data Across Partitions:}  
    Kafka divides incoming data into partitions within a topic and distributes these partitions across brokers in the cluster. This design allows efficient load balancing and supports simultaneous access by multiple consumers.

    \item \textbf{Consumers Process Data:}  
    Consumers subscribe to topics and retrieve data from specific partitions. Kafka ensures that data can be processed in parallel, enabling high-performance real-time analytics or other operations.
\end{enumerate}

\section{Computational Methods}

Managing Apache Kafka, a distributed and data-intensive system, requires careful application of computational methods. Kafka's complex architecture involves intricate interactions between brokers, topics, partitions, and consumers. Computational approaches simplify this complexity, making it feasible to develop a Text-Based User Interface (TUI) that provides users with an intuitive way to manage Kafka. This section outlines how principles such as \textit{Abstraction}, \textit{Decomposition}, \textit{Thinking Ahead}, and \textit{Algorithmic Thinking} are employed in the development of the TUI.

\subsection{Abstraction}

\subsubsection*{Definition:}  
Abstraction simplifies a complex system by emphasizing its essential features while hiding less relevant details.

\subsubsection*{Application:}  
Abstraction is at the heart of this TUI’s design, allowing users to interact with Kafka’s core components—brokers, topics, partitions, and consumer groups—without requiring deep knowledge of Kafka’s APIs or internal mechanisms. The TUI abstracts Kafka’s functionalities into high-level commands and representations:
\begin{itemize}
    \item \textbf{Brokers:} Represented as nodes with health status and connectivity information.
    \item \textbf{Topics:} Abstracted as logical containers for message streams, with partition and replication details available upon request.
    \item \textbf{Consumers:} Displayed as groups subscribing to topics, with key metrics such as consumer lag simplified for clarity.
\end{itemize}

This abstraction ensures that even novice users can perform critical Kafka operations, such as topic creation, monitoring, and deletion, using an accessible interface.

\subsection{Decomposition}

\subsubsection*{Definition:}  
Decomposition breaks a large, complex problem into smaller, manageable sub-problems.

\subsubsection*{Application:}  
The development of the TUI leverages decomposition to divide Kafka management into specific functional components:
\begin{itemize}
    \item \textbf{Topic Management:} Includes operations such as creating, deleting, and listing topics with detailed configurations.
    \item \textbf{Consumer Monitoring:} Tracks consumer groups, displays subscription details, and monitors consumer lag.
    \item \textbf{Broker Status Monitoring:} Reports on broker health and displays connectivity details.
\end{itemize}

Each component is further divided into smaller tasks. For example, ``Topic Management'' includes sub-tasks like validating topic names, setting replication factors, and handling partition assignments. Decomposition makes the project more manageable, enables iterative development, and allows for parallelization of work across different modules.

\subsection{Thinking Ahead}

\subsubsection*{Definition:}  
Thinking Ahead involves anticipating future needs and ensuring the solution is robust and scalable.

\subsubsection*{Application:}  
The TUI is designed with extensibility in mind, anticipating potential changes in Kafka’s architecture and user requirements. For instance:
\begin{itemize}
    \item The TUI supports dynamic configuration, allowing it to adapt to changes in the number of brokers or topics without significant modifications.
    \item Modular code design ensures that new features, such as support for Kafka Streams or advanced monitoring tools, can be integrated seamlessly.
\end{itemize}

By thinking ahead, the TUI ensures long-term usability and minimizes maintenance efforts, even as Kafka evolves.

\subsection{Algorithmic Thinking}

\subsubsection*{Definition:}  
Algorithmic Thinking involves designing and implementing efficient, logical steps to solve a problem.

\subsubsection*{Application:}  
Several features of the TUI rely on algorithmic thinking:
\begin{itemize}
    \item Efficient algorithms for calculating consumer lag ensure that real-time metrics are displayed without significant delays.
    \item Partition assignment algorithms optimize how messages are distributed across brokers, ensuring load balancing.
    \item Sorting and filtering operations in the TUI enable users to quickly locate relevant topics or consumers, even in large Kafka clusters.
\end{itemize}

These algorithmic enhancements ensure that the TUI provides accurate and responsive feedback, critical for managing a live distributed system.

\subsection{Evaluation and Refinement}

As part of the computational process, evaluation and refinement ensure that the TUI meets user needs effectively. Continuous testing is performed to verify:
\begin{itemize}
    \item Usability: Ensuring the interface remains intuitive for users with varying levels of experience.
    \item Performance: Verifying that the TUI scales efficiently with large Kafka clusters.
    \item Correctness: Ensuring that operations, such as topic creation or consumer monitoring, produce accurate results.
\end{itemize}

\subsection{Conclusion}

By applying computational methods such as abstraction, decomposition, thinking ahead, and algorithmic thinking, the TUI simplifies Kafka management for users. These principles ensure that the solution is user-friendly, scalable, and adaptable to future requirements, demonstrating the importance of computational methods in solving complex, real-world problems.


\chapter{Research}
\section{Interview questions}

I will outline here key questions that I will ask stakeholder, but during the interview I may come up with follow up questions. The interview will help me find their opinion on the software and how they would like to use it.

\subsection{Questions}
\begin{itemize}
    \item \textbf{How often do you work with Kafka? What utilities do you use to work with it? Why those?}
        \begin{itemize}
            \item Understanding the frequency of interaction with Kafka is crucial for designing a utility that meets the stakeholder's needs without overwhelming them with unnecessary features
        \end{itemize}
    \item \textbf{Can you describe in detail the last problem you solved related to Kafka?}
    \begin{itemize}
            \item Gathering detailed information about recent challenges helps in designing features and functionalities within the utility to address common pain points
    \end{itemize}
    \item \textbf{Is there anything you're currently dissatisfied with in the utilities you use? What would you change?}
        \begin{itemize}
            \item Discovering dissatisfaction with existing utilities guides the development of the utility by focusing on areas for improvement or enhancement
            \item Knowing desired changes informs feature prioritisation, ensuring that the utility aligns with stakeholder expectations and addresses their specific pain points effectively
        \end{itemize}
    \item \textbf{Do you have a preference for a Terminal User Interface (TUI) or web-based solution for managing Kafka? If so, what are the reasons behind your preference?}
        \begin{itemize}
            \item Understanding stakeholders' preferences for either a Terminal User Interface or web-based solution provides crucial insights into their workflow preferences and the environment in which they operate. This information ensures that the utility is developed in a manner that seamlessly integrates into their existing workflows, ultimately enhancing user experience and satisfaction
        \end{itemize}
    \item \textbf{Describe the recent problems you encountered while using <Name of the utilities>}
        \begin{itemize}
            \item Identifying areas of dissatisfaction with current utilities allows for targeted improvements in the design and functionality of the TUI utility. By addressing these pain points, the TUI utility can be tailored to better meet the needs and expectations of stakeholders, resulting in a more effective and user-friendly solution.
        \end{itemize}
        
    \item \textbf{What do you believe would be the ideal number of clusters the system should handle simultaneously? Why? How many clusters have you worked with simultaneously in the past?}
        \begin{itemize}
            \item Understanding the optimal number of clusters for concurrent support is key to ensuring the system scales seamlessly and manages resources efficiently.
        \end{itemize}
    \item \textbf{Could you explain how you currently configure connections to the clusters, and how user-friendly do you find this process?}
        \begin{itemize}
            \item Understanding how connections to the clusters are currently configured and evaluating the user-friendliness of this process can provide valuable insights, ensuring that the new utility addresses any shortcomings and provides a seamless user experience.
        \end{itemize}
    \item \textbf{When thinking about how data from the clusters is displayed, what are your main expectations? For example, how quickly data is presented by current tools, and how satisfied are you with this?}
        \begin{itemize}
            \item Understanding the expectations regarding data display from the clusters is crucial for informing development priorities and ensuring alignment with stakeholder needs and by gaining insights into these aspects, we can effectively prioritise features and enhancements within the utility
        \end{itemize}
    \item \textbf{Can you describe some common scenarios where you would use multiple clusters? For instance, comparing data across different topics or any other important scenarios you have in mind?}
        \begin{itemize}
            \item Understanding common scenarios where users would use multiple clusters is important for designing a utility that meets users’ needs effectively. By considering these scenarios, I can ensure that the utility is versatile enough to support a wide range of users workflow needs
        \end{itemize}
\end{itemize}


\section{Interview}


\subsection*{Can you please introduce yourself and describe your experience with Kafka?}

Sure. My name is Sarah Anderson, and I’m the Head of Engineering at Valerann. I’ve been with the company for nearly three years, and I’ve been working in tech for almost ten years. Regarding my experience with Kafka, I started using it about six years ago while working at a marketing technology company. They used Kafka to process large volumes of email notifications and automatically convert them into sales. Since then, I’ve worked with Kafka in about five different companies, all primarily for high-volume, real-time data processing.

Typically, my role with Kafka has involved helping companies implement and scale it from the ground up. Kafka provides excellent scalability, but transitioning from a traditional monolithic architecture to an event-driven, Kafka-based design requires significant effort. Developers often need training, and management needs to understand how to structure and scale a Kafka architecture. I’ve spent much of my time helping teams learn how to use Kafka, build systems around it, and implement it effectively.

\subsection*{How often do you work with Kafka now, and what utilities and tools do you use to manage it?}

Currently, I work with Kafka every few days—around three times a week. However, I don’t interact with the individual services as much as I used to. In terms of tools, we primarily use a tool called Faust Streaming, which was derived from an older tool called Faust (created by a company called Robinhood, not to be confused with the Bitcoin company). However, Faust is now deprecated, and we’re moving away from it. We’ve been using it as our streaming framework, similar to the Kafka Streams Java framework but written in Python—and not as efficiently. We're gradually replacing it with an in-house tool we’ve developed called CitizenK (inspired by the movie \textit{Citizen Kane} and Kafka's initial).

For managing Kafka day-to-day, we use an open-source tool from Provectus Labs that provides a free UI for administering Kafka clusters, performing tasks like cleaning up consumer groups or resetting offsets. For infrastructure management, we use Terraform to create clusters, brokers, topics, and set up initial settings.

\subsection*{Can you describe any problems you've encountered while using these tools to manage Kafka?}

One of the most common issues we face with admin tools is the lack of effective consumer group management. No tool I've found manages consumer groups as well as the Kafka CLI, but the CLI is cumbersome to use on a daily basis since it requires running bash scripts. For example, if you want to remove a topic from a consumer group, no admin tool I know of can do that; they only allow you to delete the entire consumer group or reset offsets. The CLI supports this, but no admin tool does, which is frustrating.

\subsection*{Do you prefer web tools over CLI tools? Why?}

I prefer web tools because they make onboarding new users much easier. It's harder to get people to read through extensive documentation to learn CLI tools. With a web interface, you can include visual guides like screenshots with instructions such as "click this button," which is much more intuitive. Writing documentation for CLI usage, such as which arguments to use and when, is far more complex. Admin UIs also offer the ability to limit user permissions, which is helpful. For example, tools like Conductor allow you to create user accounts with different levels of permissions, whereas with the CLI, users can do almost anything once they have access.

\subsection*{Would you prefer a Terminal User Interface, like Lazydocker, or a Web Interface?}

Personally, I prefer a web interface, although I know many developers prefer terminals. I’m one of the rare developers who enjoys working with a web interface. However, a terminal interface could benefit many people, especially if it’s easier to use than the Kafka CLI. A simplified terminal interface that eliminates the need to reference various configuration files and allows everything to be executed from a single root command would be a vast improvement.

\subsection*{Can you describe your experience working with multiple Kafka clusters simultaneously?}

We currently have seven Kafka clusters, each with both staging and production environments, which are differentiated only by name. Managing these clusters can be challenging. It would be great to have a tool that allows you to scan messages across all clusters or filter topics across clusters. Managing consumer groups across multiple clusters is also a manual process. For instance, resetting the offset of a consumer group in every cluster requires going through each cluster one by one, which is time-consuming and error-prone.

\subsection*{How do you connect to Kafka clusters, and how user-friendly is the process?}

We connect to Kafka clusters in three main ways. First, for all our services running in AWS, we use an AWS secret that contains Kafka credentials. These credentials are global, shared across all services for a particular customer. This makes things easier, although we recognize that creating separate credentials per service is something we should do but haven’t yet automated.

Second, we use an admin UI, which runs on local machines rather than in the cloud. We have an internal CLI tool that starts the Kafka UI, reconnecting it to all our clusters so we can browse them through the interface.

Third, we interact with Kafka through the CLI. We have another internal CLI tool that generates Kafka configuration files using the AWS global secret.

\subsection*{What are your expectations for how data from clusters, such as topics and consumer groups, should be displayed?}

I think flexibility is key. It would be great to customize the way data is displayed, for example, by pulling out information for a specific topic across all clusters or viewing all topics for a particular cluster. Additionally, showing statistics like the minimum and maximum lag for a particular topic across clusters, or the average size of topics, would be very useful.

\subsection*{Are you satisfied with the current tools’ representation of this data?}

No, I’m not satisfied at all. Most tools are awkward to use. They typically have separate subsections for topics and consumer groups, but navigating between them is cumbersome, and you often lose track of where you are in the interface.

\subsection*{Can you describe the last Kafka-related problem you solved?}

The last Kafka-related problem I solved was deleting topics and consumer groups. Since we use Amazon MSK to host Kafka, AWS handles most of the Kafka-specific issues that arise from maintaining clusters. Our biggest challenges are usually administrative, like keeping Kafka topics up-to-date as we change their structure and settings.

\subsection*{How important is it to sort topics by different parameters in your daily tasks?}

It would be extremely useful to sort topics by parameters like message volume or data size. Many of our services share topics, so sometimes, a service will start producing a large number of messages to a shared topic. Being able to quickly identify which topics have the highest message count or data volume would help us stay on top of this. Unfortunately, our current admin UI only allows us to sort topics by name, which isn’t very helpful.

\subsection*{What are the top three features you’d like to see in a Kafka admin UI tool?}

\begin{itemize}
    \item Better management of consumer groups that supports all the features available in the Kafka CLI.
    \item Cross-cluster topic statistics, so we can compare topics across multiple clusters and see metrics like size, number of connections, etc.
    \item The ability to limit user permissions. For instance, I don’t want everyone to be able to delete topics; only a few people should have that capability. It would be helpful if the UI could enforce this, in addition to Kafka’s own security settings.
\end{itemize}

\textbf{Insights from Interviews}: Key needs identified include comprehensive consumer group management, cross-cluster statistics, and user permissions. These priorities will directly inform Kandy’s development, focusing on user-centered functionality improvements.

\section{Research on Existing Solutions}

To develop an effective Kafka management tool that aligns with the needs of Kafka administrators and engineers, I conducted a research of existing solutions across GitHub, Habr, Reddit threads focused on Kafka, and various package repositories. This research aimed to identify current tools’ strengths and limitations, assess their suitability for a secure, scalable environment.

\subsection{Criteria for Evaluating Usability}
To assess each tool's effectiveness, I developed the following criteria, informed by Sarah’s preferences and project objectives

\begin{enumerate}
    \item \textbf{Open Source}: Determines the ability to customize and scale the tool affordably in an enterprise environment.
    \item \textbf{Cost}: Evaluates cost-effectiveness for deployment at scale, balancing proprietary options against open-source flexibility.
    \item \textbf{Interface Type}: The tool’s user interface—CLI, TUI, or web-based—affects usability, with a preference for web-based tools for accessibility and TUI for optional flexibility.
    \item \textbf{Consumer Group Management}: Ability to manage consumer groups, including tasks like targeted offset resets, affects the tool's effectiveness in data-driven operations.
    \item \textbf{Cross-Cluster Capabilities}: Supports management across multiple Kafka clusters, vital for Sarah’s team managing complex deployments.
    \item \textbf{Data Sorting and Flexibility}: Ability to sort topics by message volume, data size, and other metrics for an optimized user experience.
    \item \textbf{User Permissions}: Provides secure, granular user roles for safe, role-based access management in multi-user environments.
\end{enumerate}

\newpage

\subsection{Kafka Manager} 
\subsubsection*{Overview} 
Kafka Manager, a community-maintained open-source tool by Yahoo, offers a basic web-based interface for managing Kafka clusters. It provides standard functionality for topic visibility and consumer group management.

\paragraph{Features and Limitations Based on Criteria}
\begin{itemize} 
    \item \small \textbf{Open Source}: Yes
    \item \small \textbf{Cost}: Free 
    \item \small \textbf{Interface Type}: Web 
    \item \small \textbf{Consumer Group Management}: Limited; lacks granular control such as the ability to remove specific topics from a consumer group, an important feature for custom data pipelines.
    \item \small \textbf{Cross-Cluster Capabilities}: Lacks multi-cluster management; suitable only for single-cluster setups, making it less viable for larger, distributed teams.
    \item \small \textbf{Data Sorting and Flexibility}: Limited sorting options; topics are sortable by name only, which restricts dynamic data organization.
    \item \small \textbf{User Permissions}: Lacks user-role management, posing a security concern in multi-user environments.
\end{itemize}

\begin{figure}[h] 
\centering 
\textit{[Placeholder for image: Screenshot of Kafka Manager’s interface displaying a single cluster’s topic and consumer group information.]} 
\end{figure}

\textbf{Suitability Summary}: Kafka Manager’s lack of cross-cluster capabilities and limited consumer group control reduce its practicality for Sarah’s team. Its single-cluster design makes it less scalable, while the absence of user permissions limits security.

\subsection{Conduktor} 
\subsubsection*{Overview} 
Conduktor, a proprietary Kafka management tool, offers a robust web-based GUI with advanced querying capabilities. Its features cater to a broad range of Kafka ecosystem components, providing operators with enhanced flexibility.

\paragraph{Features and Limitations Based on Criteria}
\begin{itemize} 
    \item \small \textbf{Open Source}: No
    \item \small \textbf{Cost}: Proprietary; requires licensing, which may reduce cost-effectiveness at scale.
    \item \small \textbf{Interface Type}: Web
    \item \small \textbf{Consumer Group Management}: Provides good consumer group controls but lacks the granularity found in CLI tools for highly specific data manipulation.
    \item \small \textbf{Cross-Cluster Capabilities}: Supports multiple clusters but lacks cross-cluster statistics, which limits data insights across clusters.
    \item \small \textbf{Data Sorting and Flexibility}: Offers flexible sorting by various metrics; however, it does not support cross-cluster topic comparisons, limiting use in environments needing aggregated insights.
    \item \small \textbf{User Permissions}: Includes role-based access controls, a core requirement for Sarah’s team.
\end{itemize}

\begin{figure}[h] 
\centering 
\textit{[Placeholder for image: Conduktor’s GUI showing user permissions settings and multi-cluster management options.]} 
\end{figure}

\textbf{Suitability Summary}: Conduktor’s robust interface and user-permission management are valuable; however, its proprietary model and limited cross-cluster statistics make it less adaptable for scalable, cost-effective Kafka deployments.

\subsection{Kafdrop} 
\subsubsection*{Overview} 
Kafdrop is an open-source, web-based tool for monitoring Kafka clusters. It provides a straightforward UI, focusing on Kafka brokers, topics, and partitions, making it suitable for small to medium clusters.

\paragraph{Features and Limitations Based on Criteria}
\begin{itemize} 
    \item \small \textbf{Open Source}: Yes
    \item \small \textbf{Cost}: Free
    \item \small \textbf{Interface Type}: Web
    \item \small \textbf{Consumer Group Management}: Basic; lacks advanced control such as offset resets for specific consumer groups.
    \item \small \textbf{Cross-Cluster Capabilities}: Not supported; limited to monitoring single clusters.
    \item \small \textbf{Data Sorting and Flexibility}: Limited sorting options; cross-cluster comparisons absent.
    \item \small \textbf{User Permissions}: Does not support role-based access control, which reduces its security in multi-user scenarios.
\end{itemize}

\begin{figure}[h] 
\centering 
\textit{[Placeholder for image: Kafdrop’s UI displaying basic cluster status and topic information.]} 
\end{figure}

\textbf{Suitability Summary}: While user-friendly, Kafdrop’s limited features make it unsuitable for large-scale Kafka operations, especially those requiring in-depth consumer group management and cross-cluster capabilities.

\subsection{Kafka CLI Tools} 
\subsubsection*{Overview} 
Kafka CLI Tools, built directly into Apache Kafka, provide a set of powerful, low-level commands for various management tasks, including managing topics and consumer groups.

\paragraph{Features and Limitations Based on Criteria}
\begin{itemize} 
    \item \small \textbf{Open Source}: Yes
    \item \small \textbf{Cost}: Free
    \item \small \textbf{Interface Type}: CLI
    \item \small \textbf{Consumer Group Management}: Comprehensive; provides granular control over offsets, including removing specific topics from consumer groups.
    \item \small \textbf{Cross-Cluster Capabilities}: Limited; each cluster requires manual configuration.
    \item \small \textbf{Data Sorting and Flexibility}: Limited; lacks built-in sorting and visualization without custom scripting.
    \item \small \textbf{User Permissions}: No user-role management, complicating restricted access in shared environments.
\end{itemize}

\begin{figure}[h] 
\centering 
\textit{[Placeholder for image: Example of Kafka CLI commands being executed to manage topics and consumer groups.]} 
\end{figure}

\textbf{Suitability Summary}: Kafka CLI Tools provide high control but lack usability for daily operations, visualizations, and multi-user management, making them unsuitable for teams requiring intuitive interfaces and cross-cluster management.

\subsection{kafkactl} 
\subsubsection*{Overview} 
kafkactl, an open-source CLI tool inspired by Kubernetes’ \texttt{kubectl} syntax, is designed for teams familiar with Kubernetes looking to streamline Kafka management.

\paragraph{Features and Limitations Based on Criteria}
\begin{itemize} 
    \item \small \textbf{Open Source}: Yes
    \item \small \textbf{Cost}: Free
    \item \small \textbf{Interface Type}: CLI
    \item \small \textbf{Consumer Group Management}: Moderate; lacks full Kafka CLI capabilities.
    \item \small \textbf{Cross-Cluster Capabilities}: Not inherently supported; can be configured.
    \item \small \textbf{Data Sorting and Flexibility}: Limited; requires additional scripting for data sorting.
    \item \small \textbf{User Permissions}: No role-based management, limiting security in multi-user settings.
\end{itemize}

\begin{figure}[h] 
\centering 
\textit{[Placeholder for image: Terminal interface showing kafkactl commands managing Kafka topics in a Kubernetes-style syntax.]} 
\end{figure}

\textbf{Suitability Summary}: kafkactl is more suited to teams with existing CLI expertise but lacks essential user permissions and cross-cluster capabilities for complex deployments.

\subsection{Kafka Monitor} 
\subsubsection*{Overview} 
Kafka Monitor, an open-source tool by LinkedIn, provides monitoring for Kafka performance, primarily focusing on end-to-end latency, throughput, and reliability.

\paragraph{Features and Limitations Based on Criteria}
\begin{itemize} 
    \item \small \textbf{Open Source}: Yes
    \item \small \textbf{Cost}: Free
    \item \small \textbf{Interface Type}: CLI-based monitoring
    \item \small \textbf{Consumer Group Management}: Limited; primarily a monitoring tool.
    \item \small \textbf{Cross-Cluster Capabilities}: Lacks cross-cluster monitoring.
    \item \small \textbf{Data Sorting and Flexibility}: Primarily focused on performance metrics with minimal interactive sorting.
    \item \small \textbf{User Permissions}: No role management, limiting secure multi-user access.
\end{itemize}

\begin{figure}[h] 
\centering 
\textit{[Placeholder for image: Graph output from Kafka Monitor displaying latency and throughput metrics.]} 
\end{figure}

\textbf{Suitability Summary}: While beneficial for performance monitoring, Kafka Monitor lacks management and cross-cluster features needed for daily Kafka administration.

\newpage
\subsection{Comparative Summary}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Tool} & \textbf{Open Source} & \textbf{Cost} & \textbf{Interface} & \textbf{Consumer Group} & \textbf{Cross-Cluster} & \textbf{Data Sorting} & \textbf{Permissions} \\ \hline
Kafka Manager & Yes & Free & Web & Limited & No & Minimal & No \\ \hline
Conduktor & No & Proprietary & Web & Good & Limited & Flexible & Yes \\ \hline
Kafdrop & Yes & Free & Web & Basic & No & Minimal & No \\ \hline
Kafka CLI Tools & Yes & Free & CLI & Comprehensive & Manual & Minimal & No \\ \hline
kafkactl & Yes & Free & CLI & Moderate & Configurable & Limited & No \\ \hline
Kafka Monitor & Yes & Free & CLI & Limited & No & Minimal & No \\ \hline
\end{tabular}%
}
\caption{Comparison of Kafka Management Tools by Feature}
\end{table}

After gathering this information, it’s evident that Conduktor largely addresses Sarah's requirements.\footnote{As I later learned, \textbf{Conduktor} is indeed the solution Sarah's team uses in their development process.} It would be ideal to explore its functionality further; however, due to its proprietary nature, I was unable to do so.

The next best options to consider for development are Kafka CLI Tools and Kafka Manager. Kafka CLI Tools were developed by Confluent, the creators of Kafka, and offer a full suite of utilities to manage Kafka-related issues. However, based on Sarah's feedback and my own experience, Kafka CLI Tools can be challenging to use due to the numerous commands and flags, which are not always intuitive. Kafka Manager, on the other hand, is a free alternative to Conduktor, supporting all essential features needed for cluster management. 

\newpage
\section{Look into Other TUI Applications}

My experience in UI development is close to zero, so I decided to draw inspiration and knowledge from TUI wrappers for other services, such as:

\textbf{lazydocker}: A TUI for managing Docker containers

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{imgs/LazyDocker.png}
    \caption{LazyDocker main screen}
    \label{fig:enter-label}
\end{figure}

\textbf{Dolphie}: A utility for SQL database analytics

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\linewidth]{imgs/Dolphie.png}
    \caption{Dolphie main screen}
    \label{fig:enter-label}
\end{figure}

These are tools I personally use daily and find convenient. They allow me to quickly access necessary information. 

\subsection{LazyDocker Analysis}\label{subsec:ld_analysis}

To understand how exactly these tooks work, I analyzed LazyDocker.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{imgs/LazyDockerAnalysis.png}
    \caption{Lazydocker main screen analysis}
    \label{fig:lazydocker_analysis}
\end{figure}

When launching the utility, we can see the compact and minimalist design. All necessary categories are divided into panes, and navigation between them is performed by pressing \texttt{Tab}/\texttt{Shift+Tab} or using arrow keys. At the bottom of the screen, there’s also a bar with shortcut keys.

Panes are the most critical component of this interface. They display information, and their layout heavily influences the user experience. In the case of LazyDocker, the panes work as tables or lists of options. Beyond names, they show essential information, allowing users to navigate without guessing and compare container performance without opening detailed logs.

Additionally, there’s one large pane with detailed information about the selected container/image or network. This pane occupies more than half the screen, providing ample space for detailed data.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{imgs/LazyDockerConfigPane.png}
    \caption{LazyDocker: Config pane}
    \label{fig:ld_big_pane}
\end{figure}


This pane is divided into several tabs. Besides configuration tab (\ref{fig:ld_big_pane}), you can view statistics for each container (\ref{fig:ld_stats_screen}), environment variables (\ref{fig:ld_env_pane}), and resource usage (\ref{fig:ld_top_pane}). This pane is the primary and most convenient feature for day-to-day Docker management. I believe a similar approach could be applied to my product.

\begin{figure}[htbp]
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{imgs/LazyDockerStatsPane.png}
    \caption{Stats}
    \label{fig:ld_stats_screen}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{imgs/LazyDockerEnv.png}
    \caption{Env}
    \label{fig:ld_env_pane}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{imgs/LazyDockerConfig.png}
    \caption{Config}
    \label{fig:ld_config_pane}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{imgs/LazyDockerTopPane.png}
    \caption{Top}
    \label{fig:ld_top_pane}
\end{subfigure}
\caption{Big pane tabs}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{imgs/LazyDockerHotKeysMenu.png}
    \caption{Hotkey Menu}
    \label{fig:enter-label}
\end{figure}

Lastly, pressing a hotkey \texttt{X} opens a menu with a wide range of commands. Hotkeys are designed to speed up processes, and this kind of menu, which consolidates all necessary commands, significantly enhances efficiency. Unlike the bar, this menu allows for a detailed description of each key binding.

This is probably my favorite feature of LazyDocker because it eliminates the need to exit the utility and type lengthy commands to accomplish simple tasks and basiclly speeds up my work several times. I definitely plan to implement such a menu in my product.


\subsection{Conclusions}

After analyzing LazyDocker, I identified key features that could be useful in creating my own utility:

\begin{itemize}
    \item \textbf{Pane-based layout}: This structure looks harmonious and logical for such utilities. Each pane functions as a container, simplifying the logic of information display.
    \item \textbf{Table-like panes}: Displaying data in this format aligns with Kafka’s structure and is likely the optimal solution.
    \item \textbf{Menu and hotkey bar}: These elements make navigation through the interface a quick and effortless task, often requiring no conscious thought.
    \item \textbf{Tabbed panes}: As with the large pane, using tabs instead of separate screens can sometimes be more convenient and logical.
    \item \textbf{Colors}: Though not explicitly mentioned earlier, colors play a vital role both aesthetically and functionally. Highlighting the active pane, using a “traffic light” system to display container statuses, and syntax highlighting for configurations all enhance the user experience.
\end{itemize}

By incorporating these features, I aim to develop a TUI application that is both efficient and user-friendly.

\newpage
\section{Features of the proposed solution}
\label{sec:proposed_features}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/UserStoryMap.png}
    \label{fig:USM}
\end{figure}

To outline the initial application concept and plan for future development, I chose to use User Story Mapping (USM). This approach provides a user-centered perspective, aiding in prioritizing features across different development phases. USM is an agile approach to requirements engineering, composed of several key elements:

\begin{enumerate} \item \textbf{User Activity:} This step involves identifying the main activities users will perform within the system, representing the core actions they take to achieve their goals. \item \textbf{User Tasks:} Within each user activity, specific tasks are defined. These tasks represent the necessary steps users must follow to accomplish the activity. \item \textbf{User Stories:} Each task is further refined into user stories. These stories represent the functionalities or features needed to fulfill the task from the user's perspective. \end{enumerate}

In a User Story Map, user stories are organized into swimlanes based on their release phases. Releases indicate the prioritization of user stories, outlining which ones should be completed first. The essential structure of USM lies in its sequential arrangement of user tasks from left to right, aiming to provide a comprehensive overview of user interactions with the system. This format helps ensure a clear understanding of user needs and workflows.

One of the main advantages of USM over other requirements engineering techniques is its flexibility to accommodate changes based on user feedback. Its visual representation also aids in understanding the system from the user’s perspective, enhancing communication and collaboration among stakeholders.

On the map, tasks are divided into two releases: Topic Demo and MVP. The Topic Demo represents the initial stage, where I will create the application framework, TUI, and add the essential topic-viewing feature. This demo serves as a basis for consulting with my stakeholder and allows for adjustments to the USM if needed.

After consulting with Sarah, I will refine the application according to her feedback and proceed to work on the second release, MVP (Minimal Viable Product). This release will include additional features, such as displaying statistics across all topics and enabling interaction with consumer groups.

\section{Limitations of my solution:}

The primary limitation of this solution is time. As this project is part of the A-level coursework, alongside other assignments, my task estimates may be overly optimistic. Working independently, I may realistically only be able to implement a portion of the planned functionality.

Additionally, there are limitations associated with the libraries I intend to use. Currently, I am considering urwid or textual for developing the TUI (Text User Interface). However, urwid, for instance, does not support a broad range of terminal types and may perform inconsistently in certain environments, which could pose challenges and restrict my solution’s effectiveness.

\chapter{Requirements}

\section{Software product quality model}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/ISO25010.png}
    \label{fig:quality_model}
\end{figure}

    I based my quality requirements on the ISO 25010 standard, which outlines the key characteristics of any software product, and developed a quality model using the QAS (Quality Attribute Scenarios) methodology. To achieve this, I identified critical attributes by referencing both interviews with Sarah and industry standards. A description of the model and its attributes is provided below.
    
\newpage

\subsection{Quality Attribute Scenarios (QAS)}

Each quality attribute is defined through quality attribute scen-arios, which must adhere to the six-part rule. For more information on this method, see https://wstomv.win.tue.nl/edu/2ii45/year-0910/Software\_Architecture\_in\_Practice\_2nd\_Edition\_Chapter4.pdf

\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.3} % Set a comfortable line spacing
    \begin{adjustbox}{max width=\textwidth} % Fit table to page width
        \begin{tabular}{| p{3.5cm} | p{10.5cm} |}
            \hline
            \textbf{Term} & \textbf{Description} \\ \hline
            Stimulus & Describes an event arriving at the system, like a performance event, user operation, or security attack. A stimulus for modifiability could be a modification request, while for testability, it could be a phase completion. \\ \hline
            Stimulus Source & The origin of a stimulus, affecting how the system treats it. For example, a trusted user’s request may be handled differently from an untrusted user's. \\ \hline
            Response & Specifies how the system or developers should act in response to the stimulus, detailing responsibilities at runtime or during development. \\ \hline
            Response Measure & Defines how to judge if the response meets requirements, helping determine if the system or developer actions are sufficient. \\ \hline
            Environment & The context in which the scenario occurs, which qualifies the stimulus and influences system response. \\ \hline
            Artefact & The part of the system to which the requirement applies, often the entire system but sometimes only specific components. \\ \hline
        \end{tabular}
    \end{adjustbox}
    \caption{Definitions of Key Terms}
\end{table}

\subsection{Environment}
    I am gonna use my laptop as a \textbf{normal operating conditions} for several reasons. First of all, it's always at hand, so I don't need to worry about logistics or where I'll be testing the product. Secondly, I have a fairly average laptop in terms of specifications, so if the utility works smoothly on it, it will likely work on more powerful machines as well. Therefore, all hardware requirements are based on the basic characteristics of my device and all software requirements are based on the installed software on the my machine.

\begin{table}[h!tbp]
\centering
\begin{tabular}{| l | l |}
\hline
\textbf{Part of environment} & \textbf{Variable} \\
\hline
OS & Manjaro Linux 23.1.4 (Vulcan) \\
\hline
Terminal Emulator & Kitty 0.31.0 \\
\hline
Kafka version & confluentinc/cp-kafka:7.7.1\\
\hline
Python version & 3.11.4 \\
\hline
CPU & 1.60 GHz \\
\hline
RAM & 4 GB \\
\hline
Docker version & 25.0.3 \\
\hline
Python libs versions & Specified in the file \textbf{pyproject.toml} in~Appendix:\ref{appendix:libs} \\
\hline

\end{tabular}

\end{table}
As Kafka will be deployed on the same system where Kandy is tested, I anticipate virtually negligible latency when it comes to communicating between services


\newpage
\section{Success criteria}
\begin{table}[h!tbp]
\centering

\begin{tabular}{|p{7cm}|p{5cm}|p{3cm}|}
\hline
\textbf{Criteria} &  Justification&Evidence \\
\hline
Time behaviour QAS have passed successfully (Response time less than 10 seconds https://www.nngroup.com/articles/response-times-3-important-limits/)&  &.log file \\
\hline
 &   &\\
\hline
 &   &\\
\hline

\end{tabular}

\end{table}

\chapter{Design}

Having identified the requirements and success criteria, I can now proceed with designing the system itself. The primary goal of this process is to create a structure that facilitates the addition of new features and the maintenance of the project while preserving or improving its functionality. A thoughtful design approach is essential to ensure effective system development, maintainability, scalability, and adaptability for future changes.

Any design process begins with the decomposition of the problem into manageable pieces of work and domain-specific elements. This activity often involves brainstorming.

As a visual thinker, I decided to create a diagram that depicts the elements and sections of the system being designed.

\section{Decomposition}

The first level of elements I identified is as follows:
\begin{itemize}
    \item Define core functional requirements
    \item Choose development technology
    \item Design user interface structure
    \item Test and debug
    \item Release and support
\end{itemize}

Since I am building a tool for an existing software system, my tool is heavily dependent on its core functionality and capabilities. Therefore, understanding and defining the core functional requirements is a high-priority activity.

Choosing suitable development technology is critical for several reasons. First, it must enable technical integration with Kafka. Second, I must have sufficient expertise in the chosen technology to ensure that the project is completed within the set timeline.

Given that the project goal is to create an application with a user interface (UI) to enhance how target users interact with Kafka, I need to carefully plan the UI's structure and design to ensure an optimal user experience.

Any software application requires an appropriate level of testing. To define what ``appropriate'' means for my application, I must consider how the system will be tested and determine the level of logging required to ensure that it works as expected.

The final phase of application development involves delivering the software to the target users. The process of installation and running the application is just as important as the application requirements themselves. Thus, I aim to plan ahead and identify the tools and strategies needed to make this process as seamless as possible.

In the following sections, I will explain my thought process for each element and how they contribute to the system's development.

\newpage

\subsection{Prioritization}
Features are categorized into \textbf{essential} (green), \textbf{desirable} (yellow), and \textbf{optional} (red):
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{imgs/DesignLegend.png}
    \caption{System Design Diagram Legend}
    \label{fig:design_legend}
\end{figure}

For the full diagram, refer to \textbf{Appendix~\ref{appendix:system_design}}. Given the large size of the diagram, I will discuss its components and subdivisions separately in the following sections

\subsection{Core functional requirements}

As mentioned earlier, understanding and defining the core functional requirements is a high-priority activity. I have divided this into three main blocks:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/CoreFucntionalRequirementsDiagram.png}
    \caption{Core Functional Requirements}
    \label{fig:core_functional_requirements}
\end{figure}

\begin{itemize}
    \item \textbf{Basic Kafka Operations} — This involves core functionalities for interacting with Kafka. These are features that are essential for any Kafka management utility. I have outlined them separately in Figure~\ref{fig:basic_kafka_operations}, which I will discuss further below.
    
    \item \textbf{Intuitive Interface} — Since I am developing a terminal-based utility for developers who are accustomed to working with terminal applications like Vim, creating user-friendly navigation through versatile keyboard shortcuts is a priority. Additionally, because Kafka is inherently designed for processing large volumes of data, it is essential to provide users with the ability to quickly search for relevant information within a large data stream.
    
    \item \textbf{Multitasking and Asynchronous Workflows} — Again, considering Kafka’s nature of handling significant amounts of data, the fetching process can take time. Ensuring concurrency of processes in this scenario is necessary, and implementing background tasks will be an effective solution.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/BasicKafkaOperationsDiagram.png}
    \caption{Basic Kafka operations requirements}
    \label{fig:basic_kafka_operations}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/BasicKafkaOperationsDiagram.png}
    \caption{Basic Kafka operations requirements}
    \label{fig:basic_kafka_operations}
\end{figure}

Based on Kafka's architecture (Figure~\ref{fig:kafka_arch}), four primary features come to mind that the management utility must support:
\begin{itemize}
    \item Working with topics
    \item Managing consumer groups
    \item Interfacing with producers
    \item Cluster management
\end{itemize}

\subsection{Interface Structure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/InterfaceDesign.png}
    \caption{Interface design structure}
    \label{fig:interface_design}
\end{figure}

The technologies chosen for UI development significantly influence the design process. Therefore, I want to finalize the interface design before selecting the technology stack. In Figure~\ref{fig:interface_design}, I have highlighted the priority tasks. The most critical aspect is the main menu design, as its navigational capabilities and quick access to core features will determine whether users find the application useful. No matter how powerful the system is, if it’s inconvenient to use, users won’t engage with it.

\subsection{Main Menu Design}\label{subsec:main_menu_design}

From my analysis of LazyDocker in Section~\ref{subsec:ld_analysis}, I identified several features I would like to include in my solution.

Here, I adapt them to suit the needs of my utility:
\begin{itemize}
    \item \textbf{Panes}: Kafka data is naturally suited for display in table format, as most elements can be easily categorized.
    \item \textbf{Hotkeys}: Hotkeys are what make navigation satisfying. They are essential for any TUI utility. I plan to implement a general hotkeys bar and context-specific menus for individual panes. For example, sorting topics by different parameters can be performed using hotkeys.
    \item \textbf{Navigation}: Navigation is equally important. Moving between panes should be intuitive and straightforward. I decided to support navigation not only with \texttt{Tab}/\texttt{Shift+Tab} and arrow keys but also with vim-style hotkeys. Many developers use vim/emacs plugins in their workflows, and it’s enjoyable to have familiar keybindings across the developer environment.
    \item \textbf{Colors}: As mentioned in the analysis, colors play both an aesthetic and functional role. They help highlight important elements and focus the user’s attention where needed. Ideally, I would like to provide customizable palettes, but for the MVP, I will hardcode a basic color palette. If time permits, I will later update it to be configurable.
\end{itemize}

Inspired by LazyDocker, I sketched the following main menu mockup:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/UIMaquette.png}
    \caption{Main menu mockup}
    \label{fig:ui_maquette}
\end{figure}

The first MVP version will only include the topic view and message view. I will discuss these in detail below. For now, a brief note on consumer and broker views: when designing mockups, I drew inspiration from Kafka Manager as a free alternative to Conduktor (which was discussed by Sarah). All views include a data table with various columns. For consumer groups, an initial implementation could include only group names (IDs) and associated topics. The broker screen is intended to track load metrics, so I divided it into two panels—a table panel with general information about each broker and a detailed information panel for the selected broker. In the future, the second panel could include tabs, such as one for rendering load graphs or displaying data loss statistics. To determine their utility, I will consult with my stakeholder.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\linewidth]{imgs/TopicsScreenDesign.png}
    \caption{Topics screen mockup}
    \label{fig:topics_screen_maquette}
\end{figure}

\textbf{Topics View}: This is the main screen users will interact with most frequently, so it will be the first to be implemented for the MVP. This screen is divided into three panes:
\begin{itemize}
    \item A general pane displaying information about topics in the cluster.
    \item A metrics pane for the selected topic, potentially including graphs for specific time periods. Initially, it will display basic data.
    \item A messages pane showing the last \textit{N} messages in the selected topic. By pressing enter on any topc will open new screen with messages stored in this topic (Figure~\ref{fig:message_view}).
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\linewidth]{imgs/MessageView.png}
    \label{fig:message_view}
    \caption{Message view}
\end{figure}

Overall, this screen is not drastically different from the topics screen. It presents messages in a tabular format, and when a message is selected, its content can be viewed in an editor displayed on the right side of the screen.

\newpage
\subsection{Tech Stack}

Having finalized the two most critical parts of the interface, I can now determine the technology stack. Below, in Figure~\ref{fig:tech_chart}, I have divided the technologies into three categories.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\linewidth]{imgs/TechDiagram.png}
    \caption{Technology Stack Overview}
    \label{fig:tech_chart}
\end{figure}

\textbf{Programming Language}: Choosing the programming language is a key decision for the project, as it affects performance, coding style, and the overall development approach. I considered using Rust or Go, but I am not sufficiently experienced with these languages to confidently complete the project within the given timeframe. C++ and C were also on the rejected list for the same reason, even though they are promising choices for creating lightweight, optimized interfaces for Linux command-line environments. These languages, in my opinion, would be excellent for developing a lightweight and optimized terminal interface tailored for Linux users. However, I chose Python instead. Python is both simple to read and powerful, with an extensive library ecosystem. Additionally, I have been working with Python since I was 8 years old, which enables me to estimate tasks with a high degree of accuracy in terms of time. For these reasons, Python will form the foundation of this project.

\textbf{TUI Framework}: The next most important aspect is the user interface. After conducting research, I identified four options. \texttt{Urwid} is a relatively outdated library with sparse documentation, but from the screenshots, it seems to include all the essential components for creating an interface based on my mockups. Its downsides are that it only supports a few terminal emulators and does not run on Windows at all. Then, I discovered \texttt{Textual} and was immediately impressed with its documentation—everything is well-documented, and the library includes numerous widgets required for developing a terminal interface. Additionally, \texttt{Textual} claims to be nearly universal, running seamlessly across different environments while maintaining consistent appearance, which is crucial for a good TUI utility. You may notice that I marked not only \texttt{Textual} as my preferred framework but also another option—\texttt{Rich}. Technically, \texttt{Rich} was created by the authors of \texttt{Textual} and integrates fully with it, so I consider them as a single framework, even though they are technically distinct libraries.

I also highlighted the option of building a custom framework in yellow. This option is highly unlikely but not impossible. If \texttt{Textual} fails to meet my needs, I may have to explore other options or develop a solution myself, which would require significantly more time. However, within the scope of this coursework, it is unlikely that I will pursue this route.

\textbf{Kafka Client}: The wrapper cannot function without connecting to the system itself. I found two options, but \texttt{confluent\_kafka} has better and simpler documentation, along with a more active GitHub repository. Therefore, I chose this library. As with the TUI framework, creating a custom framework is always an option, but it is an extremely complex task and is unlikely to be part of this coursework.

\subsection{Testing and Debugging}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/TestDebugDiagram.png}
    \caption{Testing and Debugging Plan}
    \label{fig:test_debug}
\end{figure}

It is crucial to establish the testing boundaries early on to avoid spending excessive time on non-critical areas. Logging is an essential feature for any project, as debugging without logs can become a nightmare. The same applies to error highlighting—errors should be highlighted in the logs to make them easier to locate in large log files. For the evaluation phase, I need to provide proof of testing, and since I plan to use a Test-Driven Development (TDD) approach during development, tracking the percentage of code covered by tests will be helpful.

Since this is an MVP, stress testing and scalability are not critical at this stage. The MVP's goal is to function and demonstrate the system's capabilities to the stakeholders. Like a sketch in drawing, an MVP provides sufficient understanding but is almost never the final extensible solution. Therefore, I decided not to include performance benchmarking in the initial releases, and stress testing will only be conducted if time allows.

\subsection{Release and Support}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{imgs/ReleaseSupportDiagram.png}
    \caption{Release and Support Plan}
    \label{fig:release_support}
\end{figure}

Supporting the product during and after development is as important as the development itself. I divided this section into three parts: documentation, updates, and feedback/analytics.

\textbf{Documentation}: Documentation is a critical part of any project involving multiple developers. However, since this is a school project with limited time, and good documentation is typically time-intensive, I will focus on inline code comments and Python's built-in \texttt{pydoc} library (\url{https://docs.python.org/3/library/pydoc.html}), which converts comments in the code into well-formatted HTML and CLI manual pages.

\textbf{Updates}: Tracking application versions is good practice. In the final application, I could offer users the option to update the package when new versions are released. However, this task is not essential for the MVP, and I find it challenging to estimate the time required for implementing this functionality. Thus, this task will be deferred until after the core features of the first release are completed.

\textbf{Feedback and Analytics}: These are two things that help improve the application. In this project, I work iteratively with my stakeholders, collecting feedback to refine the application. I also believe that assessing an application's performance requires more than just feedback—quantifiable metrics are needed for a more accurate evaluation. However, collecting metrics from the beginning, before acquiring any users, is relatively pointless and is not critical for developing a good MVP. Therefore, assuming a small number of initial users (stakeholders), I plan to implement this feature in future releases.

\section{Structure of the Solution}

We have already discussed the interface layout in Section~\ref{subsec:main_menu_design}, but now, based on the technical decisions—particularly the choice of programming language—we can consider the design of the backend, which is the core of the application responsible for processing and transferring data from Kafka to the frontend.

In real-world development, the Domain-Driven Design (DDD) approach is widely used. DDD not only helps structure the project but also facilitates communication between technical teams and stakeholders. My stakeholders are developers, so they can generally understand technical discussions. However, using DDD will allow us to "speak the same language." For example, instead of saying, "That thing in Kafka that stores messages," I can simply say "Topic," and everyone will understand. This is the main advantage of DDD—it aligns terminology and simplifies collaboration. Additionally, DDD makes it easier to structure the project architecture.

To understand the subsequent sections, it is essential to define what a design pattern is. A \textbf{design pattern} is a frequently used solution to a common problem encountered during software architecture design. Design patterns serve as guidelines for structuring your code in a way that promotes flexibility, reusability, and scalability. To explain it in simple terms, a design pattern is like a blueprint or template for solving a specific problem. For further details, see \url{https://refactoring.guru/design-patterns/what-is-pattern}.

For the application, I decided to base the architecture on the MVC (Model-View-Controller) design pattern. MVC is a popular method of organizing code. The central idea of MVC is that each section of your code has a distinct purpose:
\begin{itemize}
    \item \textbf{Model}: Handles the data and business logic of the application. It fetches and processes data, typically from Kafka in our case.
    \item \textbf{View}: Manages the display and presentation of the data. For this project, it will be responsible for the terminal user interface (TUI).
    \item \textbf{Controller}: Acts as the intermediary between the Model and View. It processes user input, updates the Model, and refreshes the View accordingly.
\end{itemize}

The primary advantage of MVC is that it separates concerns, making the code easier to maintain and scale. Moreover, since I am following a Test-Driven Development (TDD) approach, MVC naturally supports testing by isolating the logic in the Model and keeping the View and Controller relatively lightweight.

\subsection{MVC use}

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{imgs/InitialArchitecture.png}
  \end{center}
  \caption{}\label{fig:mvc_diagram}
\end{figure}

In Figure~\ref{fig:mvc_diagram}, I present a high-level overview of how I plan to implement the MVC pattern for this project. Let us briefly examine each component:

\subsubsection{Model}

The Model will handle all interactions with Kafka. This includes:
\begin{itemize}
    \item Fetching metadata (e.g., list of topics, consumer groups).
    \item Reading and writing messages to and from topics.
    \item Monitoring cluster health and performance metrics.
\end{itemize}

\subsubsection{View}

The View is responsible for rendering the user interface. I will use the \texttt{textual} and \texttt{rich} libraries to build an intuitive terminal-based UI. The main components include:
\begin{itemize}
    \item \textbf{Main Menu}: Displays the core features and navigational options.
    \item \textbf{Data Tables}: For presenting topics, messages, and metrics in an organized format.
    \item \textbf{Message Viewer}: Provides a detailed view of individual messages, basically editor.
\end{itemize}

\subsubsection{Controller}

The Controller will handle user inputs (e.g., keyboard shortcuts, navigation commands) and translate them into actions. For example:
\begin{itemize}
    \item If a user selects a topic, the Controller will fetch the corresponding messages via the Model and update the View.
    \item If a user deletes a topic, the Controller will invoke the appropriate method in the Model and refresh the View.
\end{itemize}

By centralizing input handling in the Controller, we ensure that the business logic in the Model remains decoupled from the presentation logic in the View.

\subsection{Architecture}

\begin{figure}[htpb]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{imgs/ArchitectureDetailed.png}
  \end{center}
  \caption{Detailed Architecture Overview}\label{fig:detailed_arch}
\end{figure}

After spending some time sketching out ideas, I arrived at this architecture. All components are divided into three categories and follow the MVC pattern. The system is initialized from a bootstrap class, and public objects are stored in this class to allow them to be accessed from anywhere within the system.

\begin{itemize}
  \item \textbf{Controller}: The brains of the application, responsible for processing and dispatching signals to other systems. Includes:
    \begin{itemize}
      \item \textbf{Exceptions}: A set of custom exceptions that the system will throw in case of errors. While not critical for an MVP, they will enhance user feedback when reporting bugs and speed up development by clarifying error-handling logic.
    \end{itemize}

  \item \textbf{Model}: Holds all the data and the classes needed to access this data. Includes:
    \begin{itemize}
      \item \textbf{Configuration Class}: A class that stores configurable parameters. During initialization, it loads data like the Kafka host, login, and password from configuration files or environment variables. Centralizing all configurable parameters makes refactoring simpler by clearly separating responsibilities
      \item \textbf{Models}: Specifications for the data models the system works with. Essentially, these are data classes used to validate incoming and outgoing data
      \item \textbf{Adapters}: Adapters follow a design pattern and act as an interface to external systems. Multiple adapters could be supported, but for the MVP, I plan to focus on Kafka. The structure of the Kafka adapter is detailed in subsection~\ref{subsec:Adapter}
    \end{itemize}

  \item \textbf{View}: The frontend of the application, containing the views or displays that the user interacts with. The controller reads the user input, processes it, and displays the appropriate information through the views. There could be multiple views (e.g., web, CLI, native app), each with a different implementation but consistent functionality. I plan to implement in the first release a Topics view (TUI) and configureable parameter inputs (CLI)
\end{itemize}

\subsection{Models}

In the first release, I will focus on Topics and Messages. Therefore, the primary models will be \texttt{Topic} and \texttt{Message}. However, given that Kafka topics are divided into partitions, and messages reside in partitions rather than directly in topics, it makes sense to add \texttt{Partition} as a model. This simplifies data parsing and supports future functionality.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{imgs/Models.png}
  \end{center}
  \caption{Model Structure}\label{fig:models}
\end{figure}

The models are based on Kafka objects and initially use Kafka's native data types to avoid unnecessary data conversion overhead.

\newpage
\subsection{Adapter}\label{subsec:Adapter}

An adapter is an interface, and every interface has a specification. For example, an adapter converting from a UK plug to a European socket must always take three prongs as input and produce two as output. Regardless of the internal implementation of the adapter it follows this "rules". Similarly, here, I use an abstract class to specify the adapter interface for Kafka.

The abstract class defines the abstract methods that the business logic will interact with. An adapter class will inherit from this abstract class and provide concrete implementations of these methods. This concept is easier to understand when viewed in Figure~\ref{fig:AdapterUML}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=1\linewidth]{imgs/AdapterUML.png}
    \caption{Adapter Example}\label{fig:AdapterUML}
\end{figure}

I have outlined several potential methods for the system, but for the first release, I will focus on implementing only two: \texttt{get\_topics()} and \texttt{get\_messages()}.

\chapter{Development}
\section{Preparation}

\subsection{Folder Structure}

\begin{figure}[htbp]
\centering
\begin{BVerbatim}
.
├── kandy_kafka/
│   ├── adapters/
│   │   ├── __init__.py
│   │   └── kafka_adapter.py
│   ├── gui/
│   │   ├── __init__.py
│   │   ├── controller.py
│   │   └── views
│   │       └── topic_view.py
│   ├── bootstrap.py
│   ├── config.py
│   ├── logger.py
│   └── models.py
├── tests/
├── docker-compose.kafka.yaml
├── kandy.py
├── LICENSE
├── Makefile
├── pyproject.toml
└── README.md
\end{BVerbatim}
\caption{Project folder structure}
\end{figure}

For this project, I chose to use the `poetry` dependency manager instead of the standard `pip`. `Poetry` simplifies the management of environments and dependencies but requires a specific project structure. For instance, the code must reside in a folder named after the project unless specified otherwise in the configuration file. Additionally, the root directory must contain a \texttt{README.md} file, along with all Git-related files and scripts (e.g., test scripts). 

The core code is stored in the \texttt{kandy\_kafka/} folder. Below, I describe the purpose of each file and folder:

\begin{itemize}
  \item \textbf{README.md}: Contains project information, including a description, requirements, and installation instructions.
  \item \textbf{pyproject.toml}: Used by \texttt{poetry} to define dependencies and project metadata. A detailed version is available in Appendix~\ref{appendix:libs}.
  \item \textbf{Makefile}: A script for automating tasks using the \texttt{make} tool\footnote{\textbf{Make} is a command-line tool for executing tasks defined in a configuration file.}.
  \item \textbf{LICENSE}: Since I plan to open-source this project after my A-levels, I included the GPL license to guarantee end-user freedoms to run, study, share, and modify the software.
  \item \textbf{kandy.py}: The entry point of the application that the user will run.
  \item \textbf{docker-compose.kafka.yaml}: This file specifies the configuration for Docker Compose, which I use to set up a Kafka environment for testing.
  \item \textbf{kandy\_kafka/}: Contains the core project code:
    \begin{itemize}
      \item \textbf{adapters/}: Holds adapter and repository files.
      \item \textbf{gui/}: Contains views and the controller.
      \item \textbf{bootstrap.py}: Defines the application startup logic.
      \item \textbf{config.py}: Handles external configuration parameters.
      \item \textbf{logger.py}: Manages logging setup using Python's built-in \texttt{logging} library.
      \item \textbf{models.py}: Contains data models, represented as Python \texttt{dataclasses}.
    \end{itemize}
  \item \textbf{tests/}: Directory containing automated tests.
\end{itemize}

This folder structure enforces separation of concerns, helping to write cleaner, maintainable code.

\subsection{Libraries}

The latest version of \texttt{pyproject.toml} is provided in Appendix~\ref{appendix:libs}. For now, I have included the following libraries:

\begin{itemize}
  \item \textbf{pytest}: A testing framework for creating and running test cases efficiently.
  \item \textbf{pydantic}: A data validation library
  \item \textbf{confluent-kafka}: A library chosen during the design phase to handle Kafka cluster communication.
  \item \textbf{urwid}: Although I initially selected \texttt{textual} as the primary library for GUI development, I decided to prototype one view using \texttt{urwid} for comparison and learning purposes. This experiment will help refine my choice.
\end{itemize}

\newpage

\section{Coding}
\subsection{Project Skeleton}

I began by creating the skeleton of my application. This involves defining the classes and functions I plan to use, without implementing the actual logic yet.


\begin{code}
  \begin{minted}[fontsize=\small, breaklines]{Python}
# kandy_kafka/logger.py

import logging


def setup_logger(log_level: str):
    """
    Set up the logger for the application.
    """
    # Remove all handlers associated with the root logger object.
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    logging.basicConfig(
        format="%(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s",
        datefmt="%Y-%m-%d:%H:%M:%S",
        level=log_level,
        filename="kandy_kafka.log",
    )
  \end{minted}
\caption{Logger setup function}
\end{code}

For logging, I reused a function from one of my previous projects, making minor adjustments. The function initializes a logger and formats the log output to be written to a file with name \texttt{kandy\_kafka.log} instead of \texttt{stdout}.


\begin{code}
  \begin{minted}[fontsize=\small]{Python}
# kandy_kafka/config.py

import os
from kandy_kafka import logger


class Config:
    LOGGING_LEVEL: str

    KAFKA_HOST: str
    KAFKA_PORT: int

    def __init__(self) -> None:
        self.LOGGING_LEVEL = os.getenv("LOGGING_LEVEL", "INFO")
        logger.setup_logger(self.LOGGING_LEVEL)

        self.KAFKA_HOST = os.getenv("KAFKA_HOST", "localhost")
        self.KAFKA_PORT = int(os.getenv("KAFKA_PORT", 9092))
  \end{minted}
\caption{Configuration class}
\end{code}

Next on the line was configuration. This is a class with public fields that are filled from environment variables or configuration files at application startup. The configuration contains fields for the Kafka cluster host and port, which will later be used in the Kafka adapter. I also included the logging level in the configuration, allowing the application to adjust the detail level of logs (e.g., \texttt{ERROR}, \texttt{DEBUG}, \texttt{INFO}). Not necessary for this type of project but still I think it is a good practice.

\begin{code}
  \begin{minted}[fontsize=\small]{Python}
# kandy_kafka/adapters/kafka_adapter.py

from abc import ABC, abstractmethod


class AbstractKafkaClusterAdapter(ABC):
    """
    Abstract base class for Kafka cluster adapters.

    Defines the interface for interacting with a Kafka cluster, including fetching topics
    and retrieving messages from a given topic.
    """
    @abstractmethod
    def get_topics(self):
        raise NotImplementedError

    @abstractmethod
    def get_messages(self, topic_name: str):
        raise NotImplementedError


class KafkaAdapter(AbstractKafkaClusterAdapter):
    """
    Kafka adapter that implements the methods to interact with a Kafka cluster using
    confluent-kafka library.
    """
    def __init__(self, host: str, port: int):
        pass

    def get_topics(self):
        pass

    def get_messages(self, topic_name: str):
        pass
  \end{minted}
\caption{Kafka adapter implementation}
\end{code}

The adapter interface defines the methods required for interaction with a Kafka cluster, such as retrieving topic names and fetching messages from topics. The implementation of the \texttt{KafkaAdapter} class is currently a placeholder, as the exact details depend on the chosen library. 
The abstract class ensures that any child class implements all required methods by raising a \texttt{NotImplementedError} if a method is missing. The \texttt{KafkaAdapter} class includes an \texttt{\_\_init\_\_} method that takes the Kafka host and port as arguments, preparing it for future implementation.
\newpage

\begin{code}
  \begin{minted}[fontsize=\small]{Python}
# kandy_kafka/bootstrap.py

from dataclasses import dataclass
from kandy_kafka.adapters.kafka_adapter import KafkaAdapter
from kandy_kafka.config import Config


@dataclass
class Bootstraped:
    kafka_adapter: KafkaAdapter
    config: Config


class Bootstrap:
    bootstraped: Bootstraped

    def __call__(self):
        config = Config()
        kafka_adapter = KafkaAdapter(config.KAFKA_HOST, config.KAFKA_PORT)
        bootstraped = Bootstraped(
            kafka_adapter=kafka_adapter,
            config=config
        )

        return bootstraped
  \end{minted}
\caption{Bootstrap implementation}
\end{code}

Next, I created the bootstrap mechanism. This class initializes and orchestrates the application components, making them accessible through a single point. The \texttt{Bootstraped} dataclass contains the objects that need to be shared across the system, such as the Kafka adapter and configuration.

The \texttt{\_\_call\_\_} method initializes the system components and returns a \texttt{Bootstraped} object. This approach allows components to be accessed globally without passing them explicitly between functions, simplifying the code.

\begin{code}
  \begin{minted}[]{Python}
from kandy_kafka.bootstrap import Bootstrap

module = Bootstrap.bootstraped.module_name
  \end{minted}
\caption{Accessing modules through the bootstrap}
\end{code}

This design avoids excessive dependency injection, making the system easier to debug and maintain.

\newpage
\subsection{Configuration functionality}

In my project, I decided to use the approaches of TDD (Test-Driven Development) and BDD (Behavior-Driven Development). First, I define features using the Gherkin syntax, then I write tests that describe the functionality I want to implement, and finally, I write the code that will pass these tests. 

The first feature in my plan is related to connection configuration. I aim to simplify manual testing and provide a more comprehensive picture to my stakeholders during demos. By allowing changes to be made to the system from outside (e.g., through startup flags or a configuration file), I must strictly validate the data and throw meaningful errors if the user makes a mistake during input.

Config will look like this, it will have: alias, host and port fields. I will do it as yaml configuration as many backend developers familiar with it. I thought about writing my own config-parser for educational porpuses, but decided that I will probably sink a lot of time into this and it's not really related to project. 

\begin{code}
  \begin{minted}{yaml}
  default:
    host: localhost
    port: 9092
  \end{minted}
  \caption{Config file contents example}
\end{code}  

\begin{listing}[htbp]
  \begin{minted}[]{Gherkin}
Feature: Read and validate host configuration

  Scenario: Error when no configuration file is found
    Given Configuration file is not present
    When system loads config  
    Then application should prompt user to create or specify a configuration file
  
  Scenario Outline: Configuration file has syntax error
    Given Configuration file is present
    And Configuration file has <error_type> syntax error
    When system loads config
    Then application should show <error>

    Examples:
      | error_type   | error                  |
      | missing_host | "Host is missing"      |
      | missing_port | "Port is missing"      |
      | empty_file   | "Configuration file is empty" |

  Scenario: Configuration file is valid
    Given Configuration file is present
    And Configuration file has valid syntax
    When system loads config
    Then config should have valid connection details

  Scenario: Configuration file has invalid yaml syntax
    Given Configuration file is present
    And Configuration file has invalid yaml syntax
    When system loads config
    Then application should raise yaml error
  \end{minted}
\end{listing}

How should you approach reading this? As mentioned earlier, this is a Gherkin script, a structured language specifically designed for Behavior-Driven Development (BDD). Its primary purpose is to describe system behavior in plain text, bridging the gap between technical developers and non-technical stakeholders.

\newpage
The Gherkin script is organized into \textbf{features}, which represent high-level functionalities of the system. Each feature is further divided into \textbf{scenarios}, outlining specific use cases or tests related to that feature. The scenarios follow a clear and consistent structure:

\begin{itemize}
  \item \textbf{Given}: Sets up the initial state or preconditions required for the test.
  \item \textbf{When}: Specifies the action or event that triggers the behavior.
  \item \textbf{Then}: Describes the expected outcome or result.
\end{itemize}

Consider the following scenario:

\begin{minted}[]{Gherkin}
Scenario: Error when no configuration file is found
  Given Configuration file is not present
  When system loads config
  Then application should prompt user to create or specify a configuration file
\end{minted}

This scenario describes the behavior of the system when no configuration file is found. Each line is self-explanatory, making it accessible to developers and stakeholders alike.

For repetitive tests involving different inputs and outputs, Gherkin provides \textbf{Scenario Outlines}. These serve as templates for multiple test cases. For example:

\begin{minted}[]{Gherkin}
Scenario Outline: Configuration file has syntax error
  Given Configuration file is present
  And Configuration file has <error_type> syntax error
  When system loads config
  Then application should show <error>

  Examples:
    | error_type   | error                  |
    | missing_host | "Host is missing"      |
    | missing_port | "Port is missing"      |
    | empty_file   | "Configuration file is empty" |
\end{minted}

Here, the \texttt{<error\_type>} and \texttt{<error>} placeholders represent variables that are defined in the \texttt{Examples} section. This structure allows the same scenario to be tested with multiple conditions.

\begin{code}
  \begin{minted}[samepage=false]{Python}
import pytest
from pytest_bdd import parsers, scenarios, given, when, then
import yaml

from kandy_kafka.config import Config
from kandy_kafka.exceptions import HostsFileHasWrongSyntax, HostsFileNotFound
from pathlib import Path

scenarios("../features/hosts.feature")


# Non-existing configuration file scenario
@pytest.fixture
@given("Configuration file is not present")
def non_existing_config_file(tmp_path):
    return tmp_path / "hosts.yaml"


@pytest.fixture
@when("system loads config")
def config():
    return Config()


@then("application should prompt user to create or specify a configuration file")
def check_prompt_to_create_config(config, non_existing_config_file):
    with pytest.raises(HostsFileNotFound):
        config.hosts_file = non_existing_config_file
        config.load_hosts("default")


# Wrong syntax scenario
@pytest.fixture
@given("Configuration file is present")
def config_file(tmp_path):
    file = tmp_path / "hosts.yaml"
    file.touch()
    return file


@pytest.fixture
@given(
    parsers.parse("Configuration file has {error_type} syntax error"),
    target_fixture="config_file_with_wrong_syntax",
)
def config_file_with_wrong_syntax(config_file, error_type):
    assert config_file.exists()
    config_fixture = (
        Path("tests") / "fixtures" / "hosts" / f"wrong_syntax_{error_type}.yaml"
    )
    config_file.write_text(config_fixture.read_text())


@then(parsers.parse("application should show {error}"))
def check_promt_to_fix_syntax(config, error, config_file):
    print(error)  # TODO check that valid error message is in the stderr (Or stdout)
    with pytest.raises(HostsFileHasWrongSyntax):
        config.load_hosts("default", config_file)

# Correct syntax scenario
@pytest.fixture
@given("Configuration file has valid syntax")
def config_file_with_correct_syntax(config_file):
    config_file.write_text(
        """
        default:
            host: localhost
            port: 9092
        """
    )


@then("config should have valid connection details")
def check_config_connection_details(config, config_file):
    config.load_hosts("default", config_file)
    assert config.KAFKA_HOST == "localhost"
    assert config.KAFKA_PORT == 9092


# Invalid yaml syntax scenario
@pytest.fixture
@given("Configuration file has invalid yaml syntax")
def config_file_with_invalid_yaml_syntax(config_file):
    config_file.write_text("][")


@then("application should raise yaml error")
def check_yaml_error(config, config_file):
    with pytest.raises(yaml.YAMLError):
        config.load_hosts("default", config_file)
  \end{minted}
\end{code}

The code implements the scenarios defined in the feature file using the \texttt{pytest-bdd} plugin. 

In the BDD workflow, the \textbf{Given-When-Then} steps in the feature file correspond directly to Python functions in the test suite. These steps are implemented as pytest fixtures and assertions to validate the application's behavior.
The \texttt{scenarios} function links the Python code to the feature file. This enables each scenario in the Gherkin script to execute its corresponding test implementation.

\textbf{BDD Connection:}
Each \textbf{Given-When-Then} step in the Gherkin script is mirrored in the Python code using decorators: \texttt{@given}, \texttt{@when}, \texttt{@then}. For example, the "Missing Configuration File" scenario ensures that the application raises an error and prompts the user appropriately when no configuration file exists. Similarly, the "Valid Configuration File" scenario confirms that the configuration is parsed correctly, and the connection details are set as expected.

\subsection{Running the First Test}

Now, according to the standard procedure, I should check whether my code passes the tests. Even though I know it cannot, since I haven't written the implementation yet, I still need to verify and confirm that all the tests fail. Afterward, I can start writing the code. This is part of the Red-Green-Refactor cycle, which ensures that new functionality doesn't inadvertently affect the existing features and that all core functionalities continue to work as expected.

\newpage
For convenience, I added a command to run tests in the \textbf{Makefile}. In the future, I plan to include various levels of testing here (e.g., integration, unit, and behavior tests). For now, the \textbf{Makefile} looks as follows:

\begin{code}
  \begin{minted}[samepage]{make}
    .DEFAULT_GOAL := help

    help:
      @awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}' $(MAKEFILE_LIST)

    install: ## Install dependencies
      @poetry install

    test: ## Run tests
      @poetry run pytest

    run: ## Run the application
      @poetry run python -m kandy
  \end{minted}
  \caption{Makefile}
\end{code}

Next, we run the tests and hope they fail...

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{imgs/FirstTestFailed.png}
  \end{center}
  \caption{Initial Test Failure}
\end{figure}

After confirming that the code doesn't work, I can start implementing features. The first test checks the system's behavior when the configuration file is missing:

\begin{code}
  \begin{minted}[]{Python}
# Non-existing configuration file scenario
@pytest.fixture
@given("Configuration file is not present")
def non_existing_config_file(tmp_path):
    return tmp_path / 'hosts.yaml'

@pytest.fixture
@when('system loads config')
def config():
    return Config()

@then('application should prompt user to create or specify a configuration file')
def check_prompt_to_create_config(config, non_existing_config_file):
    with pytest.raises(HostsFileNotFound):
        config.hosts_file = non_existing_config_file
        config.load_hosts('default')
  \end{minted}
\end{code}

Let’s break down what this test does:

\begin{itemize}
  \item \textbf{Given}: No configuration file is present. I provide a reference to a temporary auto-generated empty directory.
  \item \textbf{When}: I initialize the \texttt{Config} class
  \item \textbf{Then}: I plan to throw a custom exception, which will be caught and processed in the main function to display an error message in the terminal, it just makes design little bit cleaner, that printing error and exiting right after error. I will add a new method \texttt{load\_hosts} in the \texttt{Config} class to handle the configuration file and I'm gonna call it in my test (And probably in Bootstrap or entrypoint in actual code later).
\end{itemize}

The \texttt{Config} class now includes an additional field, \texttt{hosts\_file}, which will store the path to the configuration file. The default path is \texttt{"HOME\_DIRECTORY/.config/kandy/hosts.yaml"}. In the future, I'll need to refactor this to ensure platform compatibility (e.g., for Windows), but for this prototype, the current implementation is good enough. This field will be managed by the \texttt{load\_hosts} method, which accepts optional arguments for a custom configuration path and a cluster alias.

We’ll start with the simplest part, the exception. In the \texttt{exceptions.py} file, I create a new class that inherits from \texttt{Exception}:

\begin{code}
  \begin{minted}{Python}
  class HostsFileNotFound(Exception):
    pass
  \end{minted}
\end{code}

Now, I can raise this error and handle it elsewhere and thats it. Moving on to the \texttt{Config} module, I've revised how host and port fields are loaded from environment variables. Since the user can specify these via the command line, it makes sense to accept them as arguments during configuration initialization.

\begin{code}
  \begin{minted}{Python}
  import os
  from kandy_kafka import logger
  from pathlib import Path

  from kandy_kafka.exceptions import HostsFileHasWrongSyntax, HostsFileNotFound
  import yaml

  class Config:
      LOGGING_LEVEL: str

      KAFKA_HOST: str
      KAFKA_PORT: int

      def __init__(self, host=None, port=None) -> None:
          self.LOGGING_LEVEL = os.getenv("LOGGING_LEVEL", "INFO")
          logger.setup_logger(self.LOGGING_LEVEL)

          self.hosts_file = Path.home() / ".config" / "kandy" / "hosts.yaml"

          self.KAFKA_HOST = host
          self.KAFKA_PORT = port
  \end{minted}
\end{code}

The code uses Python's built-in \texttt{pathlib} library to determine the home directory, which simplifies future porting to Windows. \texttt{host} and \texttt{port} are passed as arguments but default to \texttt{None} since data can be sourced either from the command line or a configuration file. The \texttt{Config} class method will overwrite these values if the data is from a file.

Here’s the \texttt{load\_hosts} method:

\begin{code}
  \begin{minted}{Python}
    def load_hosts(self, clustername="default", config_file=None):
        if not self.hosts_file.exists():
            raise HostsFileNotFound(f"Hosts file {self.hosts_file} not found")

        if config_file:
            self.hosts_file = config_file
  \end{minted}
\end{code}

This method only checks for the file's existence and raises an error if it doesn’t exist, using the \texttt{pathlib} library's built-in functionality.

After implementing this, the code should pass the first test. Let’s run it using \texttt{make test}...

\begin{figure}[!htbp]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{imgs/BehaviourTestPassed.png}
  \end{center}
  \caption{}\label{fig:}
\end{figure}

After ensuring that the code works as intended, we proceed to the next test. This test verifies the system's behavior when the configuration file contains syntax errors. For this purpose, I utilized fixtures, which are pre-defined files, functions, and variables that create a testing environment. In this case, I used prepared configuration files. I divided the \textit{given} step into two parts: first, I create a file in the directory, and second, I write the contents of a specific fixture into that file. Since the test’s conditions are identical to the previous one (the \texttt{@when} step), \texttt{pytest} automatically reuses it, avoiding duplication.

Unfortunately, I could not resolve the issue of validating specific error messages in \texttt{stderr}. For now, the error message is simply printed during the test execution, but I left a \texttt{\# TODO} comment for future improvements.

\begin{code}
  \begin{minted}[samepage]{Python}
# Wrong syntax scenario
@pytest.fixture
@given("Configuration file is present")
def config_file(tmp_path):
    file = tmp_path / 'hosts.yaml'
    file.touch()
    return file

@pytest.fixture
@given(
    parsers.parse("Configuration file has {error_type} syntax error"),
    target_fixture='config_file_with_wrong_syntax'
)
def config_file_with_wrong_syntax(config_file, error_type):
    assert config_file.exists()
    config_fixture = Path("tests") / "fixtures" / "hosts" / f"wrong_syntax_{error_type}.yaml"
    config_file.write_text(config_fixture.read_text())

@then(parsers.parse("application should show {error}"))
def check_prompt_to_fix_syntax(config, error, config_file):
    print(error) # TODO check that actual error message is in the stderr (Or stdout)
    with pytest.raises(HostsFileHasWrongSyntax):
        config.load_hosts('default', config_file)
  \end{minted}
\end{code}

To handle this test, I introduced a new exception, \textbf{HostsFileHasWrongSyntax}, and updated the \texttt{load\_hosts} method in the \texttt{Config} class. This method now includes three checks:

\begin{itemize}
    \item \textbf{Is the file empty?}
    \item \textbf{Does the file contain the specified \texttt{clustername}?}
    \item \textbf{Are the \texttt{host} and \texttt{port} parameters correctly assigned?}
\end{itemize}

These checks cover all the cases I aimed to address. If new errors arise during manual testing, I can easily add corresponding tests and extend the code to handle them.

\begin{code}
  \begin{minted}[samepage]{Python}
def load_hosts(self, clustername="default", config_file=None):
    if not self.hosts_file.exists():
        raise HostsFileNotFound(f"Hosts file {self.hosts_file} not found")

    if config_file:
        self.hosts_file = config_file

    with open(self.hosts_file, "r") as file:
        hosts = yaml.safe_load(file)
        if hosts is None:
            raise HostsFileHasWrongSyntax(f"Hosts file {self.hosts_file} is empty")

        if clustername not in hosts:
            raise HostsFileHasWrongSyntax(
                f"Clustername {clustername} not found in {self.hosts_file}"
            )

        cluster = hosts[clustername]
        self.KAFKA_HOST = cluster.get("host")
        self.KAFKA_PORT = cluster.get("port")

    if not self.KAFKA_HOST or not self.KAFKA_PORT:
        raise HostsFileHasWrongSyntax(
            f"Host or port not found in {self.hosts_file}"
        )
  \end{minted}
  \caption{Updated load\_hosts Method}
\end{code}

After running the updated code...

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{imgs/FinalBehaviourTest.png}
  \end{center}
  \caption{Final Test Results}
\end{figure}

Surprisingly (or perhaps not so surprisingly), all tests pass. Why? Because the test we just implemented is not a single test—it verifies multiple scenarios using the same algorithm (as many scenarios as there are variations of the \texttt{error} field in the Gherkin script) and in our case it does mean that it covers 3 ot of 6 tests. The first one test still passes as I didn't change any functionality linked to it, and the remaining two tests validate the happy path(they check that everything works without errors when a valid configuration file is provided), and scenario when YAML syntax itself is incorrect. 

At this stage, no further changes are needed, as the code passes all tests. Ideally, I should begin refactoring. However, since the current codebase is relatively small and I am still drafting the prototype, clean code, while important, is not the highest priority at the moment.

Как я и сказал выше, я хочу сделать не только поддержку конфига, но и флагов CLI, тоесть чтобы пользователь мог вводить данные например вот так

\begin{code}
  \begin{minted}{bash}
  kandy --host "127.0.0.1" --port 8080
  \end{minted}
\end{code}

Для поддержки флагов (аргументов) в питоне есть встроеная библиотека \textbf{argparse}, которую я и буду использовать.

Аргументы однозначно относятся к entrypoint категории, поэтому я их отнес в main file (kandy.py) 

\begin{code}
  \begin{minted}{Python}
from kandy_kafka.bootstrap import Bootstrap
import argparse

class ConditionalAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        if namespace.clustername:
            parser.error("clustername specified; --host and --port should not be used")
        setattr(namespace, self.dest, values)


def main():
    parser = argparse.ArgumentParser(description='Kandy Kafka')
    parser.add_argument("clustername", nargs='?', type=str, help='Cluster Name')
    parser.add_argument("--host", type=str, default=None, help='Host', action=ConditionalAction)
    parser.add_argument("--port", type=int, default=None, help='Port', action=ConditionalAction)

    args = parser.parse_args()

    if args.clustername and (args.host or args.port):
        parser.error("Clustername specified; --host and --port should not be used")
    if not args.clustername and (not args.host or not args.port):
        parser.error("If clustername is not specified, both --host and --port must be provided")

    Bootstrap()(clustername=args.clustername, host=args.host, port=args.port)

if __name__ == '__main__':
    main()
  \end{minted}
\end{code}

Для исключения подачи и clustername и host and port одновременно в конфиг я решил предотвратить эту возможность на уровне ввода, не идеальное решение, но для MVP проекта более, чем достаточно. Так же я подаю host, port и clustername в Bootstrap в виде аргументов, а это значит, что пришло время его тоже изменить под новые требования.

Новый бутстрап принимает в \_\_call\_\_ методе принимает **kwds и потом вытаскивает оттуда наши port, host и clustername. Я добавил немного логирования, оно не обязательно, но будет проще понять в некоторых конкретных случаях, на каком моменте запуска упало приложение, сократит время и количество затраченных сил на debug

\begin{code}
  \begin{minted}[samepage]{Python}
from dataclasses import dataclass
import logging
from typing import Any
from kandy_kafka.adapters.kafka_adapter import KafkaAdapter
from kandy_kafka.config import Config


@dataclass
class Bootstraped:
    config: Config
    kafka_adapter: KafkaAdapter


class Bootstrap:
    bootstraped: Bootstraped

    def __call__(self, *args: Any, **kwds: Any) -> Bootstraped:
        logging.info("ATTEMPTING TO BOOTSTRAP - loading config")
        config = Config(kwds["host"], kwds["port"])

        if kwds["clustername"]:
            config.load_hosts(kwds["clustername"])

        logging.info("ATTEMPTING TO BOOTSTRAP - creating KafkaAdapter")
        kafka_adapter = KafkaAdapter(config.KAFKA_HOST, config.KAFKA_PORT)

        Bootstrap.bootstraped = Bootstraped(
            config=config,
            kafka_adapter=kafka_adapter,
        )

        logging.info("BOOTSTRAPING Completed")

        return Bootstrap.bootstraped
  \end{minted}
\end{code}

After adding this code we run tests again, just to check that everything works as it should and now we can start application from a CLI, so first step is done

\subsection{Connecting to kafka}

Далее по плану идет работа с топиками, а это значит интеграция с кафкой. Обычно в компаниях кафка висит в облаке на специально выделенных серверах, но у меня такой роскоши нет, да и при разработке это не всегда удобно, иметь что-то на сервере, а не локально, поэтому тестировать я буду в окружении из докер контейнеров. Я уже говорил про докер в секции анализа, но думаю повториться будет не лишним. Докер - инструмент докеризации сервисов, если говорить кратко, то он позволяет запустить любой сервис там, где он сам может запуститься, что очень удобно. Так же я могу внешне и внутрене влиять на контейнеры симулируя нагрузку и различные условия, которые не смог бы потестировать на реальном сервере, поэтому я использую докер. Настройка контейнеров сама по себе простая, найти на официальном сайте образ, скопировать название, добавить в файлик и вуаля, сервис работает как новый, но как я и сказал, докер позволяет очень тонко работать с окружением и имеет кучу необязательных, но интересных параметров. Сейчас я покажу получившийся конфиг и расскажу что да как в нем работает:

\begin{code}
  \begin{minted}[fontsize=\small]{YAML}
    version: '3.8'

services:
  zookeeper:
      image: "confluentinc/cp-zookeeper"
      ports:
        - 32181:32181
      environment:
        - ZOOKEEPER_CLIENT_PORT=32181

  kafka:
      image: confluentinc/cp-kafka
      ports:
        - 9092:9092
        - 29092:29092
      healthcheck:
        test: nc -z localhost 9092 || exit -1
        start_period: 15s
        interval: 5s
        timeout: 10s
        retries: 10
      depends_on:
        - zookeeper
      environment:
        - KAFKA_BROKER_ID=1
        - KAFKA_ZOOKEEPER_CONNECT=zookeeper:32181
        - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
        - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        - KAFKA_INTER_BROKER_LISTENER_NAME = PLAINTEXT
        - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR = 1
    
  init-kafka:
    image: confluentinc/cp-kafka
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server kafka:9092 --list
      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic test --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic test2 --replication-factor 1 --partitions 1
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka:9092 --list
      "

  \end{minted}
  \caption{daredevil}
\end{code}

И так, что же тут происходит. Я запускаю кафку, и пробрасываю порты извне в контейнер, теперь к сервису можно будет обратиться по адрессу моего компьютера (в моем случае просто localhost) и портам 9092 и 29092. Потом идут переменные окружения, они обязательно нужны для запуска кафки, но есть несколько параметров, таких как тип шифрования, логин-пароль и пара параметров кафки, с которыми мне скорее всего не нужно будет работать. 
Кафка, как я описывал в секции~\ref{Kafka_architecture}, зависит на данный момент от другого сервиса, который называется zookeeper, поэтому пришлось поднимать его тоже и настраивать взаимоотношения между сервисами. Третий сервис я добавил в виде тестовой платформы, точнее платформы с тестами. Этот сервис будет заполнять кафку тестовыми данными, он завязан на основную систему и следит за ее "здоровьем" в начале, она не сможет закинуть данные в незапустившийся контейнер 

На данный момент init-kafka(Так я буду называть тестовую платформу) просто создает два пустых топика. Возможно в будущем я перейду на питоновские фикстуры, но в данный момент мне кажется такой подход более благолазурмным. Что касается питона, а точнее тестов, то они получились достаточно простыми. в будущем я добавлю проверки на сообщения и дополнительные тесты для обеих тем. Сейчас же, я немного подумал, и решил изменить подход, кафка будет настраиваться не в docker-compose, а через отдельный скриптик:

\begin{code}
  \begin{minted}[fontsize=\small]{bash}
echo -e 'Creating kafka topics'
for i in {1..3}; do
  kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic "test$i" --replication-factor 1 --partitions 1
done

echo -e 'Kafka topics:'
kafka-topics --bootstrap-server kafka:9092 --list

echo -e 'Filling test1 topic with 10 messages'
for i in {1..10}; do
  echo "Message$i" | kafka-console-producer --bootstrap-server kafka:9092 --topic test1
done

echo "askdl" >> /tmp/healthy

tail -f /dev/null

  \end{minted}
\end{code}

And I will call it in Dockerfile which will be used in docker-compose. Звучит сложно, но на самом деле такой подход немного проще, чем был до этого, т.к. я могу легко работать с логикой и вместо 30 вызовов команды "добавить сообщение", я могу просто написать цикл и вызвать его.

\begin{code}
  \begin{minted}{YAML}
FROM confluentinc/cp-kafka:latest

WORKDIR /app

COPY tests/kafka_setup.sh ./create_and_fill_topics.sh

USER root
RUN chmod +x ./create_and_fill_topics.sh

CMD ["/bin/bash", "-c", "./create_and_fill_topics.sh"]

  \end{minted}
\end{code}

\begin{code}
  \begin{minted}[samepage]{Python}
  import pytest
from kandy_kafka.adapters.kafka_adapter import KafkaAdapter


@pytest.fixture
def kafka_adapter(server):
    """Instantiate a KafkaAdapter object and return it as a fixture"""
    adapter = KafkaAdapter(server.HOST, server.PORT)
    yield adapter


def test_should_return_topic_list(kafka_adapter):
    topics = kafka_adapter.get_topics()
    assert isinstance(topics, list)
    assert all(topic in topics for topic in["test1","test2","test3"])


  \end{minted}
\end{code}

Я сделал небольшую фикстуру, которая возвращает адаптер и после прохождения или провала теста его закрывает, это называется tear-down. Так же я написал тест, который проверяет количество и соответствие топиков реальной картине. В будущем сюда добавятся проверкии сообщений,

Запустив тест и проверив, что он падает я приступил к кодингу. Структуру апдаптера задал я раньше, осталось заполнить содержимым. Почитав документацию confluent\_kafka нашел несколько способов подключения к кафке и примеры как работать с системой, на их основе я написал метод для вытаскивания топиков. В данный момент, это всего пара строк, которые возвращают только имена топиков в виде строк. Для теста этого достаточно, но позже я перепишу этот метод, чтобы он возвращал данные в виде внутреней модели (dataclass Topic), которая будет содержать не только имя, но и партишены, нагрузку, другие нужные данные 

\newpage
\begin{code}
  \begin{minted}{Python}
from abc import ABC, abstractmethod
from confluent_kafka.admin import AdminClient
from confluent_kafka import Consumer, KafkaException, TopicPartition
from datetime import datetime, timedelta

from typing import List

import logging 

class AbstractKafkaClusterAdapter(ABC):
    @abstractmethod
    def get_topics(self) -> List[str]:
        raise NotImplementedError

    @abstractmethod
    def get_messages(self, topic: str) -> List[str]:
        raise NotImplementedError

class KafkaAdapter(AbstractKafkaClusterAdapter):
    def __init__(self, host: str, port: int):
        self.admin_client = AdminClient({
            "bootstrap.servers": f"{host}:{port}"
        })
    
    def get_topics(self) -> List[str]:
        topics = self.admin_client.list_topics(timeout=10).topics
        return list(topics)
  \end{minted}
\end{code}

Тест проходит, названия топиков возвращаются корректно, но есть небольшая проблема с тестированием. 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{imgs/TestingTimings.png}
  \end{center}
  \caption{}\label{fig:}
\end{figure}

Метод, который я выбрал для заполнения кафки данными, из-за особенностей утилиты, работает extremly долго, и хоть это не влияет на продукт на прямую, это влияет на процесс разработки, я трачу минуту только на то чтобы протестриовать интеграцию, а при введении новых фичей приходится запускать тесты достаточно часто и в итоге вместо того чтобы кодить мне приходится ждать пока все соберется и протестируется. В ближайшем будущем нужно выбрать новый способ тестировать этот функционал, а пока я исключи интеграционные тесты из основного пайплайна и буду вызывать их только ручками при изменении адаптера

\subsection{GUI: Topics}

Подключение к кафке есть и данные получаются, запуск и настройка конфига есть, осталась самая главная часть, то что будет видить юзер. Начал я с urwid, создал несколько базовых классов, которые describe базовые entity, такие как панель, таблица, поисковик и так далее и раскидал их по экрану почти следуя дизайну. Это было трудно и заняло кучу времени т.к. у urwid устаревшая документация, но получилось как то так:

\begin{code}
  \begin{minted}{Python}
  from typing import List
from kandy_kafka.models import Topic

import urwid
import re

import logging

class TopicsView:
    def __init__(self, controler):
        self.controler = controler
        self.elements = {
            "topics_names": TopicsList(self),
            "topic_data": TopicDataPanel()
            }
        self.columns = urwid.Columns([
            ('weight', 1, self.elements["topics_names"].show()),
            ('weight', 1.5, self.elements["topic_data"].show())
        ], dividechars=1)

    def update_topics_names(self, topics_names: List[str]):
        self.elements["topics_names"].update_topics(topics_names)
        self.show()

    def get_topic(self, topic_name: str):
        return self.controler.get_topic(topic_name)
    
    def show(self):
        self.columns.contents[0] = (self.elements["topics_names"].show(), self.columns.options())
        self.controler.loop.draw_screen()


class TopicsList:
    def __init__(self, parent_view) -> None:
        self.parent_view = parent_view
        
        # Topics
        self.topics_names = []
        self.topics_list = urwid.SimpleFocusListWalker([])
        
        # Search Field
        self.search_field = urwid.Edit('Search: ')
        urwid.connect_signal(self.search_field, 'change', self.update_on_search)
        self.search_text = ''
        
        # UI
        self.listbox = self.FocusChangeListBox(self.topics_list, self.select_topic)
        self.layout = urwid.LineBox(urwid.Pile([self.listbox]), tlcorner='╭', trcorner='╮', blcorner='╰', brcorner='╯')
        self.last_focus = None
        self.layout = urwid.Pile([('pack', urwid.LineBox(self.search_field, tlcorner='╭', trcorner='╮', blcorner='╰', brcorner='╯')), self.layout])

    class FocusChangeListBox(urwid.ListBox):
        def __init__(self, body, on_focus_changed):
            super().__init__(body)
            self.on_focus_changed = on_focus_changed

        def keypress(self, size, key):
            focus_widget, _ = self.get_focus()
            key = super().keypress(size, key)
            if self.get_focus() != (focus_widget, _):
                self.on_focus_changed(None, None)
            return key
        
    def filter_topics(self):
        search_text = self.search_text
        logging.info(f"Filtering topics with search text: {search_text}")
        if search_text:
            pattern = re.compile(search_text, re.IGNORECASE)
        else:
            pattern = re.compile('.*')
        
        self.topics_list.clear()
        for topic_name in self.topics_names:
            if pattern.search(topic_name):
                selectable_item = urwid.SelectableIcon(topic_name, 100)
                self.topics_list.append(urwid.AttrMap(selectable_item, None, focus_map='focused'))

    def show(self):
        self.filter_topics()
        if self.last_focus is not None and self.last_focus < len(self.topics_list):
            self.topics_list.set_focus(self.last_focus)
        return self.layout
    
    def update_on_search(self, edit, new_edit_text):
        logging.info(f"Search text changed to: {new_edit_text}")
        self.search_text = new_edit_text
        self.parent_view.show()
            
    def update_topics(self, topics_names: List[str] = None):
        if topics_names is not None:
            self.topics_names = topics_names

    def get_selected_topic(self):
        if self.last_focus is not None and self.last_focus < len(self.topics_list):
            return self.topics_list[self.last_focus].get_text()[0]

    def select_topic(self, button, user_data):
        _, self.last_focus = self.topics_list.get_focus()


class TopicDataPanel:
    def __init__(self) -> None:
        self.topic_data = urwid.Text('')
        self.rounded_layout = urwid.LineBox(self.topic_data, tlcorner='╭', trcorner='╮', blcorner='╰', brcorner='╯')

    def show(self):
        return urwid.AttrMap(self.rounded_layout, "colored")

    def update(self, topic: Topic):
        data = {"name": topic.name,
                "is_internal": topic.is_internal,
                "Number of partitions": len(topic.partitions),
                "Partitions": [partition.id for partition in topic.partitions]}
        final_data = '\n'.join([f'{key}: {value}' for key, value in data.items()])
        self.topic_data.set_text(final_data)

  \end{minted}
\end{code}

Прежде чем подключать UI в основную систему я протестировал ее в отдельном локальном окне. Я захардкодил список топиков и посмотрел как UI будет реагировать на разные действия и в целом все работает хорошо кроме одной вещи. Я не смог отображать вещи в таблице, как бы я ни пытался кроме списка названий я не смог выводить информацию о топиках в любом другом формате.

Я нашел библиотеку panwid, которая решала эту проблему, но она не запустилась в моем окружении из-за разницы версий с библиотекой pydantic, которая используется и в моем проекте и в этой библиотеке, так что пришлось от нее отказаться, как и от urwid и переключится на работу с textual.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{imgs/UIFirstScreen.png}
  \end{center}
  \caption{}\label{fig:}
\end{figure}

\begin{code}
  \begin{minted}{Python}
import urwid

import random
import uuid
from typing import List

from panwid.datatable import DataTable, DataTableColumn, DataTableDivider


class RoundedBox(urwid.LineBox):
    def __init__(self, body):
        super().__init__(body, tlcorner="╭", trcorner="╮", blcorner="╰", brcorner="╯")


class HorizontalMenu(urwid.Padding):
    def __init__(self):
        menu = urwid.Columns(
            [
                urwid.SelectableIcon("Consumers"),
                urwid.SelectableIcon("Topics"),
                urwid.SelectableIcon("Brokers"),
                urwid.SelectableIcon("Exit"),
            ],
        )
        super().__init__(menu, left=2, right=100, min_width=100)


class SearchBar(RoundedBox):
    def __init__(self):
        super().__init__(urwid.Edit("Search: "))


class TopicsDataTable(RoundedBox):
    def __init__(self, topics: List):
        columns = [
            DataTableColumn(
                "topic",
                "Topic",
                width=("weight", 2),
                align="center",
                format_fn=lambda x: x.upper(),
            ),
            DataTableDivider("\N{BOX DRAWINGS DOUBLE VERTICAL}"),
            DataTableColumn(
                "partitions",
                "Partitions",
                align="center",
                width=(13),
            ),
            DataTableDivider("\N{BOX DRAWINGS DOUBLE VERTICAL}"),
            DataTableColumn(
                "brokers",
                "Brokers",
                align="center",
            ),
            DataTableDivider("\N{BOX DRAWINGS DOUBLE VERTICAL}"),
            DataTableColumn(
                "size",
                "Size",
                align="center",
            ),
        ]
        data = [
            dict(topic=topic, partitions=partitions, brokers=brokers, size=size)
            for topic, partitions, brokers, size in topics
        ]
        table = DataTable(
            columns=columns,
            data=data,
            with_scrollbar=True,
            sort_refocus=True,
            cell_selection=True,
        )
        super().__init__(urwid.Frame(table))


class TopicsList(urwid.Pile):
    def __init__(self, topics: List):
        super().__init__([("pack", SearchBar()), TopicsDataTable(topics)])


class TopicDetail(RoundedBox):
    def __init__(self, topic: str):
        body = urwid.Filler(urwid.Text(topic), valign="top")
        super().__init__(body)


class TopicsView(urwid.Columns):
    def __init__(self, topics: List):
        super().__init__([TopicsList(topics), TopicDetail("Topic detatils")])


class Main:
    def __init__(self, topics: List):
        palette = [
            ("table_row_body", "", ""),
            ("table_row_body focused", "white", "black"),
            ("table_row_body column_focused", "", "black"),
            ("table_row_body highlight", "", ""),
            ("table_row_body highlight focused", "", "black"),
            ("table_row_body highlight column_focused", "", "black"),
            ("table_row_header", "", ""),
            ("table_row_header focused", "", ""),
            ("table_row_header column_focused", "", "black"),
            ("table_row_header highlight", "", "yellow"),
            ("table_row_header highlight focused", "", "yellow"),
            ("table_row_header highlight column_focused", "", "yellow"),
            ("table_row_footer", "", "white"),
            ("table_row_footer focused", "", "dark gray"),
            ("table_row_footer column_focused", "", "black"),
            ("table_row_footer highlight", "", "yellow"),
            ("table_row_footer highlight focused", "", "yellow"),
            ("table_row_footer highlight column_focused", "", "yellow"),
        ]
        layout = urwid.Frame(body=TopicsView(topics), header=HorizontalMenu())
        self.loop = urwid.MainLoop(layout, palette=palette)

    def run(self):
        self.loop.run()


if __name__ == "__main__":
    # random data
    n = 100
    topics = [
        (
            uuid.uuid4().hex,
            random.randint(1, 10),
            random.randint(1, 10),
            random.randint(1, 10),
        )
        for _ in range(n)
    ]
    Main(topics).run()
  \end{minted}
\end{code}



\subsection{Switching to textual}


\appendix
\chapter{System Design}\label{appendix:system_design}
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/SystemDesign.png}
\end{figure}

\chapter{Files}
\section{pyproject.toml (Libraries and dependencies)}\label{appendix:libs}
\begin{listing}[htbp]
    \begin{minted}{Python}
[tool.poetry]
name = "kandy-kafka"
version = "0.0.1"
description = "Handy way to manage kafka"
authors = ["Perchinka <alexsator.lukin@gmail.com>"]
license = "GPL-2.0-only"
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
confluent-kafka = "^2.3.0"
urwid = "^2.6.10"
panwid = "^0.3.5"
pydantic = "^2.6.4"
pyyaml = "^6.0.1"
textual = "^0.81.0"

[tool.poetry.dev-dependencies]
pytest = "^8.1.1"
pytest-bdd = "^7.1.2"
pytest-cov = "^5.0.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
    \end{minted}
    \caption{pyproject.toml}
\end{listing}

\renewcommand{\bibname}{References}
\begin{thebibliography}{00}
\bibitem{b1} None yet
\end{thebibliography}

\end{document}
